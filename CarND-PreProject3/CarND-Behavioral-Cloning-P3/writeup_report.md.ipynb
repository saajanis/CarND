{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./examples/visualization.jpg \"Visualization\"\n",
    "[image2]: ./examples/grayscale.jpg \"Grayscaling\"\n",
    "[image3]: ./examples/random_noise.jpg \"Random Noise\"\n",
    "[image4]: ./examples/placeholder.png \"Traffic Sign 1\"\n",
    "[image5]: ./examples/placeholder.png \"Traffic Sign 2\"\n",
    "[image6]: ./examples/placeholder.png \"Traffic Sign 3\"\n",
    "[image7]: ./examples/placeholder.png \"Traffic Sign 4\"\n",
    "[image8]: ./examples/placeholder.png \"Traffic Sign 5\"\n",
    "[image9]: ./writeup__supporting_data/NVIDIA_CNN.png \"NVIDIA_CNN\"\n",
    "[image10]: ./writeup__supporting_data/track1_forward.png \"Sample images training\"\n",
    "[image11]: ./writeup__supporting_data/track1_forward_backward.png\n",
    "[image12]: ./writeup__supporting_data/track1_forward_backward_ForwardSlow.png \"Sample images training cropped\"\n",
    "[image13]: ./writeup__supporting_data/track1_forward_backward_ForwardSlow_forwardCorrectional.png\n",
    "[image14]: ./writeup__supporting_data/track1_forward_backward_ForwardSlow_patches.png\n",
    "[image15]: ./writeup__supporting_data/sample_center_image.jpg\n",
    "[image16]: ./writeup__supporting_data/sample_left_image.jpg\n",
    "[image17]: ./writeup__supporting_data/sample_right_image.jpg\n",
    "[image18]: ./writeup__supporting_data/sample_center_image_flipped.png\n",
    "\n",
    "[image19]: ./writeup__supporting_data/cropped_centre_image.png\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[image20]: ./writeup__supporting_data/cropped_normalized_data_plot.png\n",
    "[image21]: ./writeup__supporting_data/cropped_grayscaled_normalized_data_plot.png\n",
    "[image22]: ./writeup__supporting_data/sample_test_images.png\n",
    "\n",
    "\n",
    "## Project 3 writeup\n",
    "Here is a link to my [project repository](https://github.com/saajanis/CarND/tree/master/CarND-PreProject3/CarND-Behavioral-Cloning-P3) which contains the corresponding output too.\n",
    "\n",
    "### Rubric Points\n",
    "\n",
    "Here I will consider the rubric points individually and describe how I addressed each point in my implementation.\n",
    "\n",
    "#### 1. Files Submitted & Code information\n",
    "\n",
    "Submission includes all required files and can be used to run the simulator in autonomous mode. My project includes the following files:\n",
    "\n",
    "* model.py containing the script to create and train the model\n",
    "* drive.py for driving the car in autonomous mode\n",
    "* model.h5 containing a trained convolution neural network \n",
    "* writeup_report.md summarizing the results\n",
    "\n",
    "#### 2. Instructions for running\n",
    "\n",
    "The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model.\n",
    "\n",
    "Submission includes functional code using the Udacity provided simulator and the original drive.py file, the car can be driven autonomously around the track by executing\n",
    "\n",
    "python drive.py model.h5\n",
    "\n",
    "#### 3. Model Architecture and Training Strategy\n",
    "\n",
    "I used the architecture used by NVIDIA in their self-driving cars that was shown in the lessons. Here are the details of the architecture:\n",
    "\n",
    "The train_model() method takes a pre-existing model (or creates a new one if none is supplied) and trains the previous model on the newly supplied data to output a new model. This helped me in snapshotting the models and try different strategies for the kind of data that works.\n",
    "\n",
    "I used a CNN mixed with maxpooling. This image (taken from their [blog](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/)) underlying model architecture:\n",
    "\n",
    "![NVIDIA_CNN][image9]\n",
    "\n",
    "I chose a CNN based network architecture because they usually perform bettwr in tasks involving images.\n",
    "\n",
    "* For normalization, I use the recommended method to convert the pixel values to normalized, zero mean values. The data is normalized in the model using a Keras lambda layer.\n",
    "\n",
    "* I used a 2x2 stride for where the number of channels is seen as changing between the convolutional layers.\n",
    "\n",
    "* The model includes RELU layers to introduce nonlinearity.\n",
    "\n",
    "* The model contains a dropout layer in order to reduce overfitting.\n",
    "\n",
    "* 20% of the original data was set aside to ensure that the model does not overfit.\n",
    "\n",
    "* Successful test on the simulator ensured that the model, in fact, does generalize.\n",
    "\n",
    "* The model used an adam optimizer, so the learning rate was not tuned manually.\n",
    "\n",
    "* The optimization function was chosen to be *minimizing mean square error* for the predicted steering angle.\n",
    "\n",
    "#### 4. Training data collection\n",
    "\n",
    "While collecting data, my initial intuition was to collect good driving behaviour when everything was going right (car was in the middle of the road) as well as behavior to recover when the car starts going off track.\n",
    "\n",
    "Here are the different kinds of datasets (from track 1) that I had for training and what I found about them:\n",
    "\n",
    " * Driving forward normally (center of the road, fairly good speed):  \n",
    " \n",
    " ![track1_forward][image10]\n",
    " \n",
    " Car drove nicely in the simulator but went off-track pretty frequently.\n",
    " \n",
    " <br/><br/>\n",
    "  * Driving forward and then backward, normally:  \n",
    " \n",
    " ![track1_forward_backward][image11]\n",
    " \n",
    "  Car drove nicely in the simulator but went off-track relatively less frequently.\n",
    " \n",
    " <br/><br/>\n",
    "  * Driving forward , backward normally and then forward really slowly:  \n",
    " \n",
    " ![track1_forward_backward_ForwardSlow][image12]\n",
    " \n",
    "  Car drove nicely in the simulator and went off-track very infrequently drove off-track consistently at some patches (about 3) of the track.\n",
    " \n",
    " <br/><br/>\n",
    "  * Driving forward , backward normally and then forward really slowly. Then, I tried to add some data for recovering from when the car goes off-track:  \n",
    " \n",
    " ![track1_forward_backward_ForwardSlow_forwardCorrectional][image13]\n",
    " \n",
    "  There wasn't any overfitting here as in the previous models, but the car drove really erratically. The increase in validation error shows the same fact. I think it is because of the way I collected data. I would drive the car to the side of the road, start recording and comeback to the center very aggressively - maybe I should've come back to the center slowly, like in a real situation where I am correcting the car going off-course. I considered recollecting this data with the improved strategy, but the next model just worked - so I'll ignore this model/data.\n",
    " \n",
    " <br/><br/>\n",
    "  * Driving forward , backward normally and then forward really slowly. Then, I tested out this data in the simulator and collected more data for the patches where the car was going off track and trained some more on that data:  \n",
    " \n",
    " ![track1_forward_backward_ForwardSlow_patches][image14]\n",
    " \n",
    "  I collected more data specifically for the patches where the car drove off road and trained it into the last to previous model and voila, it gave me a model that drove the car nicely through the whole track! The output corresponds to driving with this model.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "#### 5. Creation of the Training Set and data augmentation\n",
    "\n",
    "For each of the datasets collected above, I applied the following transformations on the data before feeding it to the network:\n",
    "\n",
    "* A sample image for the car in the centre of the road looked like this:\n",
    "  <br/>\n",
    " ![sample_center_image][image15]\n",
    " <br/>\n",
    " \n",
    " And the left and right samples (which simulate an image taken from cameras mounted on the left and right of the car's dashboard) look like this:\n",
    " <br/>\n",
    " ![sample_left_image][image16]\n",
    " <br/>\n",
    " ![sample_right_image][image17]\n",
    " <br/>\n",
    " \n",
    " While the steering angle for the image at the centre stays as recorded - for the left and right images, the target steering angles are added and subtracted respectively with a *correction of 0.25* so we can treat them as if they were the centre image.\n",
    "<br/>\n",
    "* I then applied flipping to each image to generate a laterally inverted image (to simulate driving in the opposite direction) which will help the model generalize.\n",
    " <br/>\n",
    " ![sample_center_image_flipped][image18]\n",
    " <br/>\n",
    "\n",
    "<br/>\n",
    "* Also, built into the model is a cropping mechanism that crops out the 70 pixels from the top (the sky, shrubs etc.) and 25 pixels from the bottom (hood of the car) from each image in the training and testing dataset. They'll confuse the model rather than help and provide no useful information while making gthe decision if every image has those pixels.\n",
    "  <br/>\n",
    "  ![cropped_centre_image][image19]\n",
    "  <br/>\n",
    "\n",
    "<br/>\n",
    "* The model also has a layer to normalize the data to zero mean and small values.\n",
    " \n",
    "\n",
    "#### 6. Results\n",
    "\n",
    "The result is the following video of the car driving itself successfuly on track 1 (click on the thumbnail):\n",
    "\n",
    "[![Track_1_youtube_video](https://i.ytimg.com/vi/16tdcVb8rtE/2.jpg?time=1500788879230)](https://youtu.be/16tdcVb8rtE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0a4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
