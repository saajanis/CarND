{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./examples/visualization.jpg \"Visualization\"\n",
    "[image2]: ./examples/grayscale.jpg \"Grayscaling\"\n",
    "[image3]: ./examples/random_noise.jpg \"Random Noise\"\n",
    "[image4]: ./examples/placeholder.png \"Traffic Sign 1\"\n",
    "[image5]: ./examples/placeholder.png \"Traffic Sign 2\"\n",
    "[image6]: ./examples/placeholder.png \"Traffic Sign 3\"\n",
    "[image7]: ./examples/placeholder.png \"Traffic Sign 4\"\n",
    "[image8]: ./examples/placeholder.png \"Traffic Sign 5\"\n",
    "[image9]: ./writeup__supporting_data/labels_histogram.png \"Labels histogram\"\n",
    "[image10]: ./writeup__supporting_data/sample_images_training.png \"Sample images training\"\n",
    "[image11]: ./writeup__supporting_data/original_normalized.png\n",
    "[image12]: ./writeup__supporting_data/sample_images_training_cropped.png \"Sample images training cropped\"\n",
    "[image13]: ./writeup__supporting_data/sample_images_training_cropped_normalized.png\n",
    "[image14]: ./writeup__supporting_data/sample_images_training_cropped_grayscaled.png\n",
    "[image15]: ./writeup__supporting_data/sample_images_training_cropped_grayscaled_normalized.png\n",
    "\n",
    "\n",
    "## Project 2 writeup\n",
    "Here is a link to my [project code](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/Traffic_Sign_Classifier.ipynb) with the corresponding output.\n",
    "\n",
    "### Data set summary and exploration\n",
    "\n",
    "#### 1. Data set(s) summary.\n",
    "\n",
    "I used the numpy library conbined with python's native methods to calculate summary statistics of the traffic signs data set. Here are the summaries:\n",
    "\n",
    "* Number of training examples = 34799\n",
    "* Shape of an example image (in all datasets) = (32, 32, 3)\n",
    "* Number of validation examples = 4410\n",
    "* Number of testing examples = 12630\n",
    "* Number of examples in my custom generated testing set = 5\n",
    "* Number of classes = 43\n",
    "\n",
    "#### 2. Data visualization.\n",
    "\n",
    "##### Here's a histogram showing the distribution of the number of examples of each kind available in the datasets:\n",
    "(if the x-axis labels look a bit off, find the nearest bar to the tick)\n",
    "##### With an initial gaze, the number of examples of each type look pretty evenly distributed in each dataset (this is the last time we'll look at any statistic about the test set before we test our model). \n",
    "\n",
    "![labels_histogram][image9]\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "##### Here are some randomly sampled images from the training dataset (arranged in the increasing order of their number of occurences) labelled with the name of the traffic sign and the number of times that sign appears in the training dataset:\n",
    "\n",
    "![sample_images_training][image10]\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "### Design and Test a Model Architecture\n",
    "\n",
    "\n",
    "#### 1. Data set(s) preparation.\n",
    "\n",
    "#### Transformations:\n",
    "I decided to apply a few transformations to the data and test out my model's performance on each one of them. For each transformation, I'm including a few images after that transformation was applied to show what they look like (this is for general intuition -- I don't think it matters for how the neural net operates).\n",
    "*Similar transformations were applied to images in each dataset*\n",
    "\n",
    "**1. Normalization:**\n",
    "* I took each dataset, computed the mean pixel value for all the images in the dataset and used it to normalize every pixel in every image in the dataset using the formula new_pixel_value = ((old_pixel_value - mean_pixel_value)/mean_pixel_value). This is with an expectation to make it easier for the model to work with the images since the mean of the pixels in the dataset is closer to zero now. Here's what the training dataset looks like now:\n",
    "\n",
    "![original_normalized][image11]\n",
    "\n",
    "<br/>\n",
    "**2. Cropping the original images:**\n",
    "* The provided dataset(s) have a 'coords' property which gives information about a bounding box around the traffic sign within the image. I cropped out the part specified by the bounding box in each of train, test and validation dataset and resized the image to 32*32 by padding it with black pixels. This should assist the neural network with translational variance in the images. For my testing dataset of 5 images, I purposefully did something similar by only taking a tightly bounded picture from the internet. Here's what the training dataset looks like now:\n",
    "\n",
    "![sample_images_training_cropped][image12]\n",
    "\n",
    "<br/>\n",
    "**3. Normalization on the cropped images:**\n",
    "* It's the same normalization, except that it is applied on the cropped images:\n",
    "\n",
    "![sample_images_training_cropped_normalized][image13]\n",
    "\n",
    "<br/>\n",
    "**4. Grayscaling the cropped images:**\n",
    "* Grayscaling will reduce the number of channels from 3 to 1 for each \"pixel\". This may work out if the model is too sensitive to slight variations in conditions under which images were shot etc.. \n",
    "\n",
    "![sample_images_training_cropped_grayscaled][image14]\n",
    "\n",
    "<br/>\n",
    "**5. Normalized grayscaled cropped images:**\n",
    "* This is the same kind of normalization applied to the previous dataset. \n",
    "\n",
    "![sample_images_training_cropped_grayscaled_normalized][image15]\n",
    "\n",
    "**Other things I would've tried if I had more time:**\n",
    "* I was very curious to explore how generating more training images with the signs rotated by varying degrees would impact model performance. Sub-sampling takes care of translational variance but the model may still be sensitive to rotation.\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "#### 2. Model architecture.\n",
    "## TODO: <insert model type>\n",
    "\n",
    "My final model consisted of the following layers:\n",
    "\n",
    "| Layer         \t\t|     Description\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| Input         \t\t| 32x32x3 for RGB image / 32x32x1 for grayscale | \n",
    "| Convolution 5x5/5x5   | 1x1 stride, VALID padding, outputs 28x28x6 \t|\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Max pooling\t      \t| 2x2 stride, VALID padding, outputs 14x14x6    |\n",
    "| Convolution 5x5\t    | 1x1 stride, SAME  padding, outputs 10x10x6 \t|\n",
    "| Max pooling\t      \t| 2x2 stride, VALID padding, outputs  5x 5x6    |\n",
    "| Flatten\t      \t    | outputs 400                                   |\n",
    "| Fully connected\t\t| outputs 120       \t\t\t\t\t\t\t|\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Fully connected\t\t| outputs 84        \t\t\t\t\t\t\t|\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Fully connected\t\t| outputs 43        \t\t\t\t\t\t\t|\n",
    "| Logit          \t\t| outputs 43        \t\t\t\t\t\t\t|\n",
    "| Softmax\t\t\t\t| outputs 43        \t\t\t\t\t\t\t|\n",
    " \n",
    "\n",
    "Further, I used the AdamOptimizer for optimization operation with an optimization objective of reducing the mean cross-entropy between one-hot actual labels and max softmax probability of the model with a learning rate of 0.001.\n",
    "\n",
    "The accuracy is calculated as the fraction of correct predictions.\n",
    "\n",
    "I am training the model over 500 EPOCHS and a BATCH_SIZE of 128.\n",
    " \n",
    "#### 3. Results.\n",
    "\n",
    "**Training, validation and test:**\n",
    "### Here are the plots of training and validation accuracies while using the different transformations of the dataset. Each one of the datasets produced models that could achieve at least 93% accuracy over 500 epochs. The curves, as seen in the plots below (and raw values that I saw) had mostly flattened out with only minor variance in accuracy over each other Epoch - so I wasn't too hopeful of the performance to incrase with more training iterations.\n",
    "\n",
    "**Training accuracy:**\n",
    "*Training dataset:* 100% on all datasets.\n",
    "*Validation dataset:* Over 93% on all datasets. Over 94% on the \"Cropped images dataset\", the  \"Cropped_grayscale_data\" and the \"Cropped grayscaled normalized dataset\".\n",
    "*Test dataset:* *92%* for the original dataset, *92.2%* for the cropped grayscaled dataset, *92.6%* for the cropped dataset, *92.7%* for the original normalized dataset, *93.1%* for the cropped grayscaled normalized dataset, *93.7%* for the cropped normalized dataset.\n",
    "\n",
    "All the results are very comparable and satisfactory and too statistically insignificant to declare a winner here. LeNet architecture proved to be the right choice for this problem, mainly because of the similarity of this problem to the hand-written digit recognition problem where eNet has already proven to work well. Our problem has similar characteristics where translational invariance, finding hidden features etc. will help the network classify better. I also believe that the order of the number of classes between handwritten digit recognition problem and our problem is comparable, which helps teh architecture work in our favour but I have no evidece to prove that LeNet won't perform well when the number of classes is huge.\n",
    "\n",
    "The higher accuracy on the training dataset as compared to the validation and testing dataset points to overfitting, but since we have gotten the accuracy values we wanted, I'll move on.\n",
    "\n",
    "## TODO: Point out the specific section\n",
    "The attached ipynb has the relevant outputs.\n",
    "\n",
    "**Other things I would've tried if I had more time:**\n",
    "* The model architecture is the same as from the LeNet lab in the course. I did not have to alter it to achieve the required acccuracy on the validation set. However, I believe that choosing a more informed (through triall and error) kernel size after analyzing the images in the dataset, considering dropout techniques, dirferent kinds of padding techniques etc. could have improved the performance and are worth exploring.\n",
    "\n",
    "## TODO: Insert plots here\n",
    " \n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Test a Model on New Images\n",
    "\n",
    "####1. Choose five German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify.\n",
    "\n",
    "Here are five German traffic signs that I found on the web:\n",
    "\n",
    "![alt text][image4] ![alt text][image5] ![alt text][image6] \n",
    "![alt text][image7] ![alt text][image8]\n",
    "\n",
    "The first image might be difficult to classify because ...\n",
    "\n",
    "####2. Discuss the model's predictions on these new traffic signs and compare the results to predicting on the test set. At a minimum, discuss what the predictions were, the accuracy on these new predictions, and compare the accuracy to the accuracy on the test set (OPTIONAL: Discuss the results in more detail as described in the \"Stand Out Suggestions\" part of the rubric).\n",
    "\n",
    "Here are the results of the prediction:\n",
    "\n",
    "| Image\t\t\t        |     Prediction\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| Stop Sign      \t\t| Stop sign   \t\t\t\t\t\t\t\t\t| \n",
    "| U-turn     \t\t\t| U-turn \t\t\t\t\t\t\t\t\t\t|\n",
    "| Yield\t\t\t\t\t| Yield\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| 100 km/h\t      \t\t| Bumpy Road\t\t\t\t\t \t\t\t\t|\n",
    "| Slippery Road\t\t\t| Slippery Road      \t\t\t\t\t\t\t|\n",
    "\n",
    "\n",
    "The model was able to correctly guess 4 of the 5 traffic signs, which gives an accuracy of 80%. This compares favorably to the accuracy on the test set of ...\n",
    "\n",
    "####3. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction. Provide the top 5 softmax probabilities for each image along with the sign type of each probability. (OPTIONAL: as described in the \"Stand Out Suggestions\" part of the rubric, visualizations can also be provided such as bar charts)\n",
    "\n",
    "The code for making predictions on my final model is located in the 11th cell of the Ipython notebook.\n",
    "\n",
    "For the first image, the model is relatively sure that this is a stop sign (probability of 0.6), and the image does contain a stop sign. The top five soft max probabilities were\n",
    "\n",
    "| Probability         \t|     Prediction\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| .60         \t\t\t| Stop sign   \t\t\t\t\t\t\t\t\t| \n",
    "| .20     \t\t\t\t| U-turn \t\t\t\t\t\t\t\t\t\t|\n",
    "| .05\t\t\t\t\t| Yield\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| .04\t      \t\t\t| Bumpy Road\t\t\t\t\t \t\t\t\t|\n",
    "| .01\t\t\t\t    | Slippery Road      \t\t\t\t\t\t\t|\n",
    "\n",
    "\n",
    "For the second image ... \n",
    "\n",
    "### (Optional) Visualizing the Neural Network (See Step 4 of the Ipython notebook for more details)\n",
    "####1. Discuss the visual output of your trained network's feature maps. What characteristics did the neural network use to make classifications?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0a4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
