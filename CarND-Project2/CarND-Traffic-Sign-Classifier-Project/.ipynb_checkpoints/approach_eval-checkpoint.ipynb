{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test images analysis\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "import collections\n",
    "import math\n",
    "    \n",
    "def build_my_test():\n",
    "    test_images = listdir('./test_images/')\n",
    "    test_images_array = []\n",
    "    test_images_labels_array = []\n",
    "    for test_image_filename in test_images:\n",
    "        img = Image.open('./test_images/' + test_image_filename).convert('RGB')\n",
    "        img = img.resize((32,32), PIL.Image.ANTIALIAS)\n",
    "        test_images_array.append(np.asarray(img))\n",
    "        test_images_labels_array.append((test_image_filename.split('_')[1].split('.')[0]))\n",
    "        \n",
    "#     print(test_images_array)\n",
    "#     print(test_images_labels_array)\n",
    "    return (test_images_array, test_images_labels_array)\n",
    "\n",
    "import random\n",
    "\n",
    "def show_sample_images(Xs, count, signnames_dict, make_random=True, labels=None, fig_title=None, CMAP=None):\n",
    "    col_count = 5\n",
    "    rows = math.ceil(float(len(Xs)) / float(col_count))\n",
    "    fig = plt.figure(figsize=(4*int(col_count),2*rows))\n",
    "    if fig_title:\n",
    "        fig.suptitle(fig_title, fontsize=8)\n",
    "    for i in range(count):\n",
    "        if make_random:\n",
    "            index = random.randint(0, len(Xs)-1)\n",
    "            image = Xs[index].squeeze()\n",
    "        else:\n",
    "            image = Xs[i].squeeze()\n",
    "        ax1 = fig.add_subplot(rows,col_count,i+1)\n",
    "        ax1.set_title(labels[i] + ' occurences', fontsize=8)\n",
    "        ax1.set_xticks([]) \n",
    "        ax1.set_yticks([]) \n",
    "        ax1.imshow(image, cmap=CMAP)\n",
    "\n",
    "def build_feature_label_ranked_dict(X, Y):\n",
    "    assert(len(X) == len(Y))\n",
    "    label_feature_dict = {}\n",
    "    for x,y in zip(X, Y):\n",
    "        if y not in label_feature_dict:\n",
    "            label_feature_dict[y] = x\n",
    "    \n",
    "    label_count_dict = defaultdict(int)\n",
    "    for x,y in zip(X, Y):\n",
    "        label_count_dict[y] += 1\n",
    "    \n",
    "    ranked_count_to_label_feature_dict = collections.OrderedDict()\n",
    "    for label, count in sorted(label_count_dict.items(), key=lambda x:x[1]):\n",
    "        ranked_count_to_label_feature_dict[str(signnames_dict[str(label)]) + \": \" + str(count)] = label_feature_dict[label]\n",
    "    \n",
    "    return ranked_count_to_label_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/traffic-signs-data/train.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d31e93eaf1ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtesting_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"traffic-signs-data/test.p\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/traffic-signs-data/train.p'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import csv\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "training_file = DATA_DIR + \"traffic-signs-data/train.p\"\n",
    "validation_file = DATA_DIR + \"traffic-signs-data/valid.p\"\n",
    "testing_file = DATA_DIR + \"traffic-signs-data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "(X_my_test, y_my_test) = build_my_test()    \n",
    "    \n",
    "X_train, X_train_coords, X_train_sizes, y_train = np.asarray(train['features'], dtype=np.float32), train['coords'], train['sizes'], train['labels']\n",
    "X_valid, X_valid_coords,X_valid_sizes, y_valid = np.asarray(valid['features'], dtype=np.float32), valid['coords'], valid['sizes'], valid['labels']\n",
    "X_test, X_test_coords, X_test_sizes, y_test = np.asarray(test['features'], dtype=np.float32), test['coords'], test['sizes'], test['labels']\n",
    "\n",
    "# for k,v in build_feature_label_ranked_dict(X_train, y_train):\n",
    "#     print (k)\n",
    "#     print (v)\n",
    "#################\n",
    "\n",
    "### Replace each question mark with the appropriate value. \n",
    "### Use python, pandas or numpy methods rather than hard coding the results\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: Number of validation examples\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = X_train.shape[1:]\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Prepping Original_data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/original_data.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-70cb1bd51f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# original_data = saveAndRetrievePickle(noop_image, original_data, 'saveAndRetrieve', 'original_data')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0moriginal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaveAndRetrievePickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoop_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retrieve'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'original_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m## My data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-70cb1bd51f4e>\u001b[0m in \u001b[0;36msaveAndRetrievePickle\u001b[0;34m(transformFunc, featuresDict, mode, pickleFileName)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'saveAndRetrieve'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'retrieve'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpickleFileName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mnew_features_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/original_data.pickle'"
     ]
    }
   ],
   "source": [
    "def transformFeatures(transformFunc, XFeatures):\n",
    "    return np.array([transformFunc(X_image) for X_image in XFeatures])\n",
    "\n",
    "def saveAndRetrievePickle(transformFunc, featuresDict, mode, pickleFileName):\n",
    "    if mode == 'saveAndRetrieve':\n",
    "        X_train_features = np.array([transformFunc(X_train_image) for X_train_image in featuresDict['X_train']])\n",
    "        X_valid_features = np.array([transformFunc(X_valid_image) for X_valid_image in featuresDict['X_valid']])\n",
    "        X_test_features = np.array([transformFunc(X_test_image) for X_test_image in featuresDict['X_test']])\n",
    "             \n",
    "        new_features_dict = {'X_train': X_train_features, 'y_train': y_train, 'X_valid': X_valid_features,\\\n",
    "                            'y_valid': y_valid, 'X_test': X_test_features, 'y_test': y_test}\n",
    "    \n",
    "        with open(DATA_DIR + pickleFileName + '.pickle', 'wb') as handle:\n",
    "            pickle.dump(new_features_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      \n",
    "    if mode == 'saveAndRetrieve' or mode == 'retrieve':\n",
    "        with open(DATA_DIR + pickleFileName + '.pickle', 'rb') as handle:\n",
    "            new_features_dict = pickle.load(handle)\n",
    "    \n",
    "    return new_features_dict\n",
    "            \n",
    "   \n",
    "# Original_data\n",
    "def noop_image(image):\n",
    "    return np.asarray(image, dtype=np.float32) \n",
    "\n",
    "print(\"# Prepping Original_data\")\n",
    "original_data = {'X_train': X_train, 'y_train': y_train, 'X_valid': X_valid, 'y_valid': y_valid, 'X_test': X_test, 'y_test': y_test}\n",
    "\n",
    "# original_data = saveAndRetrievePickle(noop_image, original_data, 'saveAndRetrieve', 'original_data')\n",
    "original_data = saveAndRetrievePickle(noop_image, original_data, 'retrieve', 'original_data') \n",
    "\n",
    "## My data\n",
    "original_data['X_my_test'] = transformFeatures(noop_image, X_my_test)\n",
    "original_data['y_my_test'] = y_my_test\n",
    "##\n",
    "\n",
    "# Create cropped data\n",
    "#TODO(saajan): Reconsider zeroing out irrelevant area over resizing cropped image\n",
    "def extract_bounds_and_rescale(image, coord, size):\n",
    "    transformed_x = 32\n",
    "    transformed_y = 32\n",
    "    original_x = size[0]\n",
    "    original_y = size[1]\n",
    "      \n",
    "    x_multiplier = float(transformed_x)/float(original_x)\n",
    "    y_multiplier = float(transformed_y)/float(original_y)\n",
    "      \n",
    "    transformed_coord = (coord[0]* x_multiplier, coord[1] * y_multiplier, coord[2] * x_multiplier, coord[3] * y_multiplier)\n",
    "    transformed_coord = [int(np.rint(val)) for val in transformed_coord]\n",
    "      \n",
    "    ret_image = image.copy()\n",
    "    shape = image.shape\n",
    "     \n",
    "    ret_image[0:transformed_coord[0],:] = (0,0,0)\n",
    "    ret_image[:,0:transformed_coord[1]] = (0,0,0)\n",
    "    ret_image[transformed_coord[2]:shape[1],:] = (0,0,0)\n",
    "    ret_image[:,transformed_coord[3]:shape[0]] = (0,0,0)\n",
    "    #show_sample_images([ret_image], 1)\n",
    "    return np.asarray(ret_image, dtype=np.float32)\n",
    "\n",
    "\n",
    "# # extract_bounds_and_rescale Xs\n",
    "# X_train = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_train, X_train_coords, X_train_sizes)])\n",
    "# X_valid = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_valid, X_valid_coords,X_valid_sizes)])\n",
    "# X_test = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_test, X_test_coords, X_test_sizes)])\n",
    "  \n",
    "print(\"# Prepping Cropped_data\")\n",
    "# cropped_data = {'X_train': X_train, 'y_train': y_train, 'X_valid': X_valid, 'y_valid': y_valid, 'X_test': X_test, 'y_test': y_test}\n",
    "# with open(DATA_DIR + 'cropped_data.pickle', 'wb') as cropped_data_handle:\n",
    "#     pickle.dump(cropped_data, cropped_data_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "with open(DATA_DIR + 'cropped_data.pickle', 'rb') as cropped_data_handle:\n",
    "    cropped_data = pickle.load(cropped_data_handle)\n",
    "     \n",
    "## My data -- noop\n",
    "cropped_data['X_my_test'] = X_my_test\n",
    "cropped_data['y_my_test'] = y_my_test\n",
    "##\n",
    "# show_sample_images(cropped_data['X_train'], 10)\n",
    "\n",
    "\n",
    "# normalize Xs\n",
    "def normalize(image, mean_pixel):\n",
    "    result = (np.asarray(image, dtype=np.float32) - mean_pixel) / mean_pixel\n",
    "    #normalizer_func = np.vectorize(lambda val: (float(val)-float(mean_pixel))/float(mean_pixel))\n",
    "    return np.asarray(result, dtype=np.float32)\n",
    "\n",
    "print(\"# Prepping Original_normalized_data\")\n",
    "# original_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in original_data['X_train']])), original_data, 'saveAndRetrieve', 'original_normalized_data')\n",
    "original_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in original_data['X_train']])), original_data, 'retrieve', 'original_normalized_data')\n",
    "## My data\n",
    "original_normalized_data['X_my_test'] = transformFeatures(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in original_data['X_my_test']])), original_data['X_my_test'])\n",
    "original_normalized_data['y_my_test'] = y_my_test\n",
    "##\n",
    "\n",
    "print(\"# Prepping Cropped_normalized_data\")\n",
    "# cropped_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_data['X_train']])), cropped_data, 'saveAndRetrieve', 'cropped_normalized_data')\n",
    "cropped_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_data['X_train']])), cropped_data, 'retrieve', 'cropped_normalized_data')\n",
    "## My data\n",
    "cropped_normalized_data['X_my_test'] = transformFeatures(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_data['X_my_test']])), cropped_data['X_my_test'])\n",
    "cropped_normalized_data['y_my_test'] = y_my_test\n",
    "##  \n",
    "\n",
    "# # Experiments\n",
    "# vals = []\n",
    "# for image in cropped_normalized_data['X_train']:\n",
    "#     vals.append((np.mean(image)))\n",
    "# print(np.mean(vals))    \n",
    "\n",
    "\n",
    "import cv2\n",
    "# convert_to_grayscale Xs\n",
    "def convert_to_grayscale(image):\n",
    "    return np.asarray(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY), dtype=np.float32).reshape((32, 32, 1))\n",
    "    \n",
    "print(\"# Prepping Cropped_grayscale_data\")\n",
    "# cropped_grayscale_data = saveAndRetrievePickle(convert_to_grayscale, cropped_data, 'saveAndRetrieve', 'cropped_grayscale_data')\n",
    "cropped_grayscale_data = saveAndRetrievePickle(convert_to_grayscale, cropped_data, 'retrieve', 'cropped_grayscale_data') \n",
    "## My data\n",
    "cropped_grayscale_data['X_my_test'] = transformFeatures(convert_to_grayscale, cropped_data['X_my_test'])\n",
    "cropped_grayscale_data['y_my_test'] = y_my_test\n",
    "##\n",
    "\n",
    "print(\"# Prepping Cropped_grayscaled_normalized_data\")\n",
    "# cropped_grayscaled_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_grayscale_data['X_train']])), cropped_grayscale_data, 'saveAndRetrieve', 'cropped_grayscaled_normalized_data')\n",
    "cropped_grayscaled_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_grayscale_data['X_train']])), cropped_grayscale_data, 'retrieve', 'cropped_grayscaled_normalized_data') \n",
    "## My data\n",
    "cropped_grayscaled_normalized_data['X_my_test'] = transformFeatures(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_grayscale_data['X_my_test']])), cropped_grayscale_data['X_my_test'])\n",
    "cropped_grayscaled_normalized_data['y_my_test'] = y_my_test\n",
    "##\n",
    "\n",
    "#Naming\n",
    "original_data['name'] = 'original_data'\n",
    "original_normalized_data['name'] = 'original_normalized_data'\n",
    "cropped_data['name'] = 'cropped_data'\n",
    "cropped_grayscale_data['name'] = 'cropped_grayscale_data'\n",
    "cropped_normalized_data['name'] = 'cropped_normalized_data'\n",
    "cropped_grayscaled_normalized_data['name'] = 'cropped_grayscaled_normalized_data'\n",
    "\n",
    "#cmap\n",
    "original_data['cmap'] = None\n",
    "original_normalized_data['cmap'] = None\n",
    "cropped_data['cmap'] = None\n",
    "cropped_grayscale_data['cmap'] = 'gray'\n",
    "cropped_normalized_data['cmap'] = None\n",
    "cropped_grayscaled_normalized_data['cmap'] = 'gray'\n",
    "all_data = [original_data, original_normalized_data, cropped_data, cropped_normalized_data, cropped_grayscale_data, cropped_grayscaled_normalized_data] \n",
    "#all_data = [original_normalized_data, cropped_grayscale_data] \n",
    "#all_data = [original_normalized_data] \n",
    "\n",
    "print('Done with data prep!')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b521289f1b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of training examples =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of validation examples =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of testing examples =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image data shape =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of classes =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "\n",
    "##################\n",
    "\n",
    "\n",
    "### Data exploration visualization code goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "# Visualizations will be shown in the notebook.\n",
    "#TODO(saajan): uncomment line below and plot and do more visualization\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "#bins = np.linspace(-5, 5, 25, endpoint=True)\n",
    "\n",
    "signnames_dict = {}\n",
    "signnames = [None]\n",
    "with open('./signnames.csv') as signnamescsv:\n",
    "    signnamesreader = csv.reader(signnamescsv, delimiter=',')\n",
    "    for label, name in signnamesreader:\n",
    "        signnames.append(name)\n",
    "        signnames_dict[label] = name\n",
    "ax.hist(y_train, alpha=0.5, label='training_labels', bins=10)\n",
    "ax.hist(y_valid, alpha=0.5, label='validation_labels', bins=10)\n",
    "ax.hist(y_test, alpha=0.5, label='test_labels', bins=10)\n",
    "ind = np.arange(10)  # the x locations for the groups\n",
    "width = 0\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(tuple(signnames[1:]))\n",
    "#ax.tick_params(axis='x', which='major', labelsize=10, direction='')\n",
    "#ax.tick_params(axis='x', direction='out', colors='r')\n",
    "plt.xticks(rotation=90, linespacing=0, size=8)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "###################\n",
    "        #ax1.imshow(image, cmap=\"gray\")\n",
    "        \n",
    "        #plt.imshow(image, cmap=\"gray\")\n",
    "        #plt.imshow(image)\n",
    "           \n",
    "# show_sample_images(X_train, 10)\n",
    "\n",
    "### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n",
    "### converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample images for each class and each transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c00b2615e4f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mshow_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mshow_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": [
    "def show_data(all_data, data_suffix_key):\n",
    "    for data_dict in all_data:\n",
    "        title = 'Showing images from ' + data_dict['name'] + ' dataset for ' + data_suffix_key + ' data' + \":\"\n",
    "        feature_label_original_data = build_feature_label_ranked_dict(data_dict['X_' + data_suffix_key], data_dict['y_' + data_suffix_key])\n",
    "        show_sample_images(list(feature_label_original_data.values()), len(list(feature_label_original_data.values())), signnames_dict, make_random=False, labels=list(feature_label_original_data.keys()), fig_title=title, CMAP=data_dict['cmap'])\n",
    "    \n",
    "\n",
    "show_data(all_data, 'train')\n",
    "show_data(all_data, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this as a refernce for recognizing keys to datasets:\n",
    "#### original_data => Original images\n",
    "#### original_normalized_data => Normalized images\n",
    "#### cropped_data => Cropped images\n",
    "#### cropped_grayscale_data => Cropped grayscaled images\n",
    "#### cropped_normalized_data => Cropped normalized images\n",
    "#### cropped_grayscaled_normalized_data => Cropped grayscaled normalized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model generation and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cipher10...\n",
      "Stored 'current_data_dict' (dict)\n",
      "Stored 'current_data_dict_name' (str)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2/2 [00:29<00:00, 14.44s/epochs]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAACeCAYAAABwxA4NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHVdJREFUeJzt3Xt0VdW59/HvQ7hELoISqpUgMCgKIZAQcrCCiogIagcc\nBSo3FShSqVhb1FOOOrSlZ4xiX/XgBbG+VhCUpHhDziuK1WrFQ6kE5SKgJUDUAGIIyB0l8Lx/7J1t\nEnLZSXayQvL7jLHHyFp7rrmePQnryZpr7jnN3REREQlKo6ADEBGRhk2JSEREAqVEJCIigVIiEhGR\nQCkRiYhIoJSIREQkUEpEIiISKCUiadDM7D0z22dmzYKORaShUiKSBsvMOgGXAg4Mq8XzNq6tc4mc\nDpSIpCG7CVgFzAduLtxpZmeY2cNm9rmZ7TezD8zsjPB7l5jZSjP7xsy+NLMJ4f3vmdnkInVMMLMP\nimy7md1mZluALeF9j4brOGBma8zs0iLl48zsHjPbamYHw+93MLM5ZvZw0Q9hZkvN7Nc10UAitUGJ\nSBqym4AXwq8hZnZOeP9DQB+gH3A28B/ASTPrCLwBPA60A1KBtZU4378DFwFJ4e3V4TrOBhYBL5pZ\nfPi96cAY4BrgTGAScAR4DhhjZo0AzCwBuDJ8vMhpSYlIGiQzuwToCCx29zXAVmBs+AI/CbjD3Xe4\n+wl3X+nu3wJjgbfdPcPdj7t7vrtXJhH9wd33uvtRAHd/PlxHgbs/DDQDLgyXnQzc5+6feci6cNkP\ngf3AoHC50cB77r67mk0iEhglImmobgbecvc94e1F4X0JQDyhxFRShzL2R+vLohtmdpeZbQ53/30D\ntA6fv6JzPQeMD/88HlhYjZhEAqeHptLghJ/3/BSIM7OvwrubAW2AHwLHgC7AuhKHfgn0LaPaw0Dz\nItvnllImMtV9+HnQfxC6s9no7ifNbB9gRc7VBfiklHqeBz4xsxSgO7CkjJhETgu6I5KG6N+BE4Se\n1aSGX92BFYSeGz0LPGJm54UHDVwcHt79AnClmf3UzBqbWVszSw3XuRa43syam9mPgJ9VEEMroADI\nAxqb2f2EngUVegb4vZl1tZBeZtYWwN1zCT1fWgi8XNjVJ3K6UiKShuhmYJ67f+HuXxW+gCeAccAM\nYAOhi/1e4EGgkbt/QWjwwJ3h/WuBlHCd/w18B+wm1HX2QgUxLAfeBP4FfE7oLqxo190jwGLgLeAA\n8GfgjCLvPwf0RN1yUg+YFsYTOf2Y2WWEuug6uv4Ty2lOd0QipxkzawLcATyjJCT1QYWJyMyeNbOv\nzay0h6aE+68fM7NsM1tvZmmxD1NEAMysO/ANoUEVswMORyQmorkjmg8MLef9q4Gu4dcUYG71wxKR\n0rj7Zndv4e793P1A0PGIxEKFicjd3yf0YLYsw4EF4S/drQLamNkPYxWgiIjUb7H4HlF7io/2yQ3v\n21WyoJlNIXTXRIsWLfp069YtBqcXEZGgrVmzZo+7t6vKsbX6hVZ3fxp4GiA9Pd2zsrJq8/QiIlJD\nzOzzqh4bi1FzOwhNR1IoMbxPRESkQrFIREuBm8Kj534M7Hf3U7rlRERESlNh15yZZQCXAwlmlgs8\nADQBcPengGWEvm2eTWia+ok1FayIiNQ/FSYidx9TwfsO3BaziEREpEHRzAoiIhIoJSIREQmUEpGI\niARKiUhERAKlRCQiIoFSIhIRkUApEYmISKCUiEREJFBKRCIiEiglIhERCZQSkYiIBEqJSEREAqVE\nJCIigYoqEZnZUDP7zMyyzWxGKe+fb2bvmtnHZrbezK6JfagiIlIfVZiIzCwOmANcDSQBY8wsqUSx\n+4DF7t4bGA08GetARUSkformjqgvkO3u29z9OyATGF6ijANnhn9uDeyMXYgiIlKfVbgwHtAe+LLI\ndi5wUYkyvwXeMrPbgRbAlTGJTkRE6r1YDVYYA8x390RCy4YvNLNT6jazKWaWZWZZeXl5MTq1iIic\nzqJJRDuADkW2E8P7ivoZsBjA3f8BxAMJJSty96fdPd3d09u1a1e1iEVEpF6JJhGtBrqaWWcza0po\nMMLSEmW+AAYBmFl3QolItzwiIlKhChORuxcA04DlwGZCo+M2mtlMMxsWLnYncIuZrQMygAnu7jUV\ntIiI1B/RDFbA3ZcBy0rsu7/Iz5uA/rENTUREGgLNrCAiIoFSIhIRkUApEYmISKCUiEREJFBKRCIi\nEiglIhERCZQSkYiIBEqJSEREAqVEJCIigVIiEhGRQCkRiYhIoJSIREQkUEpEIiISKCUiEREJVFSJ\nyMyGmtlnZpZtZjPKKPNTM9tkZhvNbFFswxQRkfqqwvWIzCwOmAMMBnKB1Wa2NLwGUWGZrsB/Av3d\nfZ+Z/aCmAhYRkfolmjuivkC2u29z9++ATGB4iTK3AHPcfR+Au38d2zBFRKS+iiYRtQe+LLKdG95X\n1AXABWb2v2a2ysyGllaRmU0xsywzy8rLy6taxCIiUq/EarBCY6ArcDkwBvi/ZtamZCF3f9rd0909\nvV27djE6tYiInM6iSUQ7gA5FthPD+4rKBZa6+3F33w78i1BiEhERKVc0iWg10NXMOptZU2A0sLRE\nmSWE7oYwswRCXXXbYhiniIjUUxUmIncvAKYBy4HNwGJ332hmM81sWLjYciDfzDYB7wJ3u3t+TQUt\nIiL1h7l7ICdOT0/3rKysQM4tIiKxZWZr3D29KsdqZgUREQmUEpGIiARKiUhERAKlRCQiIoFSIhIR\nkUApEYmISKCUiEREJFBKRCIiEiglIhERCZQSkYiIBEqJSEREAqVEJCIigVIiEhGRQEWViMxsqJl9\nZmbZZjajnHIjzMzNrEozsIqISMNTYSIyszhgDnA1kASMMbOkUsq1Au4A/hnrIEVEpP6K5o6oL5Dt\n7tvc/TsgExheSrnfAw8Cx2IYn4iI1HPRJKL2wJdFtnPD+yLMLA3o4O6vl1eRmU0xsywzy8rLy6t0\nsCIiUv9Ue7CCmTUCHgHurKisuz/t7ununt6uXbvqnlpEROqBaBLRDqBDke3E8L5CrYBk4D0zywF+\nDCzVgAUREYlGNIloNdDVzDqbWVNgNLC08E133+/uCe7eyd07AauAYe6eVSMRi4hIvVJhInL3AmAa\nsBzYDCx2941mNtPMhtV0gCIiUr81jqaQuy8DlpXYd38ZZS+vflgiItJQaGYFEREJVFR3RCLS8Bw/\nfpzc3FyOHdNXA+V78fHxJCYm0qRJk5jVqUQkIqXKzc2lVatWdOrUCTMLOhypA9yd/Px8cnNz6dy5\nc8zqVdeciJTq2LFjtG3bVklIIsyMtm3bxvwuWYlIRMqkJCQl1cTvhBKRiNRJ+fn5pKamkpqayrnn\nnkv79u0j2999911UdUycOJHPPvus3DJz5szhhRdeiEXIAOzevZvGjRvzzDPPxKzO+s7cPZATp6en\ne1aWvvMqUldt3ryZ7t27Bx0GAL/97W9p2bIld911V7H97o6706hR3fmb+vHHH2fx4sU0bdqUd955\np8bOU1BQQOPGwTzmL+13w8zWuHuVZtSpO/96IiJRyM7OJikpiXHjxtGjRw927drFlClTSE9Pp0eP\nHsycOTNS9pJLLmHt2rUUFBTQpk0bZsyYQUpKChdffDFff/01APfddx+zZ8+OlJ8xYwZ9+/blwgsv\nZOXKlQAcPnyYESNGkJSUxMiRI0lPT2ft2rWlxpeRkcHs2bPZtm0bu3btiux//fXXSUtLIyUlhauu\nugqAgwcPcvPNN9OrVy969erFkiVLIrEWyszMZPLkyQCMHz+eqVOn0rdvX+655x5WrVrFxRdfTO/e\nvenfvz9btmwBQknq17/+NcnJyfTq1Ysnn3ySt956i5EjR0bqfeONNxg1alS1/z1iQaPmRKRiv/oV\nlHHhrbLUVAgngMr69NNPWbBgAenpoT/AZ82axdlnn01BQQEDBw5k5MiRJCUVXzZt//79DBgwgFmz\nZjF9+nSeffZZZsw4dZ1Pd+fDDz9k6dKlzJw5kzfffJPHH3+cc889l5dffpl169aRlpZWalw5OTns\n3buXPn36MGrUKBYvXswdd9zBV199xdSpU1mxYgUdO3Zk7969QOhOr127dqxfvx5355tvvqnws+/a\ntYtVq1bRqFEj9u/fz4oVK2jcuDFvvvkm9913H3/5y1+YO3cuO3fuZN26dcTFxbF3717atGnDtGnT\nyM/Pp23btsybN49JkyZVtulrhO6IROS006VLl0gSgtBdSFpaGmlpaWzevJlNmzadcswZZ5zB1Vdf\nDUCfPn3Iyckpte7rr7/+lDIffPABo0ePBiAlJYUePXqUemxmZiY33HADAKNHjyYjIwOAf/zjHwwc\nOJCOHTsCcPbZZwPw9ttvc9tttwGhQQBnnXVWhZ991KhRka7Ib775hhEjRpCcnMxdd93Fxo0bI/Xe\neuutxMXFRc7XqFEjxo0bx6JFi9i7dy9r1qyJ3JkFTXdEIlKxKt651JQWLVpEft6yZQuPPvooH374\nIW3atGH8+PGlDi9u2rRp5Oe4uDgKCgpKrbtZs2YVlilLRkYGe/bs4bnnngNg586dbNu2rVJ1NGrU\niKLP7kt+lqKf/d5772XIkCH84he/IDs7m6FDh5Zb96RJkxgxYgQAN9xwQyRRBU13RCJyWjtw4ACt\nWrXizDPPZNeuXSxfvjzm5+jfvz+LFy8GYMOGDaXecW3atImCggJ27NhBTk4OOTk53H333WRmZtKv\nXz/effddPv/8c4BI19zgwYOZM2cOEOoS3LdvH40aNeKss85iy5YtnDx5kldffbXMuPbv30/79qF1\nSufPnx/ZP3jwYJ566ilOnDhR7HwdOnQgISGBWbNmMWHChOo1CvDU37eycuueatejRCQip7W0tDSS\nkpLo1q0bN910E/3794/5OW6//XZ27NhBUlISv/vd70hKSqJ169bFymRkZHDdddcV2zdixAgyMjI4\n55xzmDt3LsOHDyclJYVx48YB8MADD7B7926Sk5NJTU1lxYoVADz44IMMGTKEfv36kZiYWGZcv/nN\nb7j77rtJS0srdhf185//nHPPPZdevXqRkpISSaIAY8eOpXPnzlxwwQWhHe5w8iScOAEFBXD8OHz3\nHXz7LRw7BkePwpEjcPgwHDoEBw+G9r/5Jr2+2MS0eatY+eeXqtW+UQ3fNrOhwKNAHPCMu88q8f50\nYDJQAOQBk9z98/Lq1PBtkbqtLg3fDlpBQQEFBQXEx8ez5V//4qohQ9jy6ac0josLXchPo9et//Vf\nXNyzJzf/5CehfVWwec8euoeft608vyfThs9g/fw7dhUcyDuvKvVV+IzIzOKAOcBgIBdYbWZL3b3o\nvenHQLq7HzGzqcAfgRuqEpCI1DN14OJb3dehAwcYdOutFJw4gbvzpzvvpPGGDTXfdmaVfzVqVOZ7\nqddcw1mtW/PYgw9Cs2ZVq98M4uJg5Upo0oR+TZowfuMh/vPFNj+s6seMZrBCXyDb3beF2sUygeFA\nJBG5+7tFyq8Cxlc1IJEGz/37LpLjx4v/XNVXVeq45RZo0uT7C3JhbNG8Tp4sflxtifGFu/DVJiGB\nNX/9a9Uv3FV5FX6eGFpbyrOtKomPh969AVi5dQ/Pb/2YE4e/2VXBUWWKJhG1B74ssp0LXFRO+Z8B\nb1Q1IJFKcw/1b1f3whuLi3cs6qjkSK1qiYsLJZvSXjffHHpOUNrFsbSLdxQX9GJ1VPX48urQ3Hi1\nauXWPUxb9DFPjO1N//v37KxqPTEdvm1m44F0YEAZ708BpgCcf/75sTy1VFbJC/fpfvGuLY0alX3h\nLuvVrBm0bFn544q+Gjeu3vFl1VHehXvzZtAzIinH+tz9PDG2N/26JFSrnmgS0Q6gQ5HtxPC+Yszs\nSuBeYIC7f1taRe7+NPA0hAYrQCijrs/dz60DulQy9Fp28mTNXXyDuHjXVpeJWeUvmE2bQvPmNXPh\nrW4ddWhOM5Ggxeq6HU0iWg10NbPOhBLQaGBs0QJm1hv4EzDU3b+O6szHjrHyr6uZ9sEenvi3lvDB\nrrp9AT95snItWx1VuWA2b15zfzVXp4468oU5Eam7KkxE7l5gZtOA5YSGbz/r7hvNbCaQ5e5Lgf8D\ntAReDK9V8YW7Dyuv3t2f72La/2zhiddm0W9mNUefVOXiGR9f890eVTk+Lk793CLSoET1jMjdlwHL\nSuy7v8jPV1b2xF+3PJtfd2lOv8d+X70LuC7cIoF76u9b6ZXYutizgup2uw8cOJAZM2YwZMiQyL7Z\ns2fz2WefMXfu3DKPa9myJYcOHWLnzp388pe/5KWXTv2y5eWXX85DDz1UbL66kmbPns2UKVNo3rw5\nANdccw2LFi0qNjN2daSmptKtWzcyMzNjUt/pLLC55n7QqhnPH2jBj5Or/6BLRILVK7F1ZPRUvy4J\nxUZTVdWYMWPIzMwslogyMzP54x//GNXx5513XqlJKFqzZ89m/PjxkUS0bNmyCo6I3ubNmzlx4gQr\nVqzg8OHDxeaPi6Ug1yyqjMCevJ5zZjxPjO3NtEUfx2SuIhEJTr8uCZH/z4+89VmxpFRVI0eO5PXX\nX4+sxpqTk8POnTu59NJLOXToEIMGDSItLY2ePXvy2muvnXJ8Tk4OycnJABw9epTRo0fTvXt3rrvu\nOo4ePRopN3Xq1MhaRg888AAAjz32GDt37mTgwIEMHDgQgE6dOrFnT+ha9cgjj5CcnExycnJkLaOc\nnBy6d+/OLbfcQo8ePbjqqquKnaeojIwMbrzxRq666qpisWdnZ3PllVeSkpJCWloaW7duBUJT/vTs\n2ZOUlJTI0hWXX345hbPT7Nmzh06dOgGhOeeGDRvGFVdcwaBBg8ptqwULFkSmAbrxxhs5ePAgnTt3\n5nh4JOqBAweKbdeYwhUOa/vVp08fd3f/3+w8n/tetotI3bJp06ZKH/Pw8k+942/+nz+8/NOYxHDt\ntdf6kiVL3N39D3/4g995553u7n78+HHfv3+/u7vn5eV5ly5d/OTJk+7u3qJFC3d33759u/fo0SMU\n18MP+8SJE93dfd26dR4XF+erV692d/f8/Hx3dy8oKPABAwb4unXr3N29Y8eOnpeXF4mlcDsrK8uT\nk5P90KFDfvDgQU9KSvKPPvrIt2/f7nFxcf7xxx+7u/uoUaN84cKFpX6uCy64wD///HNfvny5/+Qn\nP4ns79u3r7/yyivu7n706FE/fPiwL1u2zC+++GI/fPhwsXgHDBgQ+Qx5eXnesWNHd3efN2+et2/f\nPlKurLb65JNPvGvXrpHPWFh+woQJ/uqrr7q7+5/+9CefPn36KfGX9rtBaMxAlfJB4GNR+3VJqPtD\nt0WkQiu37uH5f37BL6/4Ec//84uY9HQUds9BqFtuzJgxQOgP6HvuuYdevXpx5ZVXsmPHDnbv3l1m\nPe+//z7jx4cmfClcDbXQ4sWLSUtLo3fv3mzcuLHUmbWL+uCDD7juuuto0aIFLVu25Prrr49MVtq5\nc2dSU1OBstc8ysrKIiEhgfPPP59Bgwbx8ccfs3fvXg4ePMiOHTsiE6fGx8fTvHlz3n77bSZOnBjp\nIixcy6g8gwcPjpQrq63+9re/MWrUKBISEorVO3nyZObNmwfAvHnzmDhxYoXnq67AE5GInP6KPhOa\nftWFMet2Hz58OO+88w4fffQRR44coU+fPgC88MIL5OXlsWbNGtauXcs555xT6hpEFdm+fTsPPfQQ\n77zzDuvXr+faa6+tUj2FCtcygrLXM8rIyODTTz+lU6dOdOnShQMHDvDyyy9X+lyNGzfmZPhrJeWt\nWVTZturfvz85OTm89957nDhxItK9WZOUiESk2kp+w77wmdH63P3Vqrdly5YMHDiQSZMmRe6GILQO\nzw9+8AOaNGlSbJ2fslx22WUsWrQIgE8++YT169cDoWcgLVq0oHXr1uzevZs33vh+drJWrVpx8ODB\nU+q69NJLWbJkCUeOHOHw4cO8+uqrXHrppVF9npMnT7J48WI2bNgQWbPotddeIyMjg1atWpGYmMiS\nJUsA+Pbbbzly5AiDBw9m3rx5HDlyBPh+baFOnTqxZs0agHIHZZTVVldccQUvvvgi+fn5xeoFuOmm\nmxg7dmyt3A2BEpGIxMCtA7qcMjAhVt3uY8aMYd26dcUS0bhx48jKyqJnz54sWLCAbt26lVvH1KlT\nOXToEN27d+f++++P3FmlpKTQu3dvunXrxtixY4utZTRlyhSGDh0aGaxQKC0tjQkTJtC3b18uuugi\nJk+eTO/e0Y0OXLFiBe3bt+e8875fLeGyyy5j06ZN7Nq1i4ULF/LYY4/Rq1cv+vXrx1dffcXQoUMZ\nNmwY6enppKam8tBDDwFw1113MXfuXHr37h0ZRFGastqqR48e3HvvvQwYMICUlBSmT59e7Jh9+/YV\na/OaFNV6RDVB6xGJ1G1aj6jheumll3jttddYuHBhqe+X9rthZmvcvewvZpWj7g8wFxGRWnP77bfz\nxhtvxPR7UxVRIhIRkYjHH3+81s+pZ0QiUqaguu6l7qqJ3wklIhEpVXx8PPn5+UpGEuHu5OfnEx8f\nH9N61TUnIqVKTEwkNzeXvLy8oEOROiQ+Pp7ExMSY1hlVIjKzocCjhJaBeMbdZ5V4vxmwAOgD5AM3\nuHtOTCMVkVrVpEkTOnfuHHQY0gBU2DVnZnHAHOBqIAkYY2ZJJYr9DNjn7j8C/ht4MNaBiohI/RTN\nM6K+QLa7b3P374BMYHiJMsOB58I/vwQMMtMiQSIiUrFoElF74Msi27nhfaWWcfcCYD/QNhYBiohI\n/VargxXMbAowJbz5rZl9UpvnP40lAFq0qWJqp+ipraKntorOhVU9MJpEtAPoUGQ7MbyvtDK5ZtYY\naE1o0EIx7v408DSAmWVVdTqIhkZtFR21U/TUVtFTW0XHzKo8Z1s0XXOrga5m1tnMmgKjgaUlyiwF\nbg7/PBL4m+vLByIiEoUK74jcvcDMpgHLCQ3fftbdN5rZTEIr8i0F/gwsNLNsYC+hZCUiIlKhqJ4R\nufsyYFmJffcX+fkYMKqS5366kuUbMrVVdNRO0VNbRU9tFZ0qt1Ngy0CIiIiA5poTEZGAKRGJiEig\najQRmdmzZvZ1Wd8XspDHzCzbzNabWVpNxlOXRdFW48JttMHMVppZSm3HWFdU1FZFyv2bmRWY2cja\niq0uiaadzOxyM1trZhvN7O+1GV9dEsX/v9Zm9j9mti7cVhNrO8a6wMw6mNm7ZrYp3A53lFKm0tf1\nmr4jmg8MLef9q4Gu4dcUYG4Nx1OXzaf8ttoODHD3nsDvadgPUOdTflsVzpH4IPBWbQRUR82nnHYy\nszbAk8Awd+9B5Qcc1SfzKf936jZgk7unAJcDD4e/ztLQFAB3unsS8GPgtlLmHq30db1GE5G7v09o\nOHdZhgMLPGQV0MbMfliTMdVVFbWVu690933hzVWEvljcIEXxewVwO/Ay8HXNR1Q3RdFOY4FX3P2L\ncHm1VTlFgFbhOTRbhssW1EZsdYm773L3j8I/HwQ2c+qUb5W+rgf9jCiaeezkVD8D3gg6iLrKzNoD\n19Gw77CjcQFwlpm9Z2ZrzOymoAOqw54AugM7gQ3AHe5+MtiQgmVmnYDewD9LvFXp67oWxjvNmNlA\nQonokqBjqcNmA79x95OaBL5cjQmtITYIOAP4h5mtcvd/BRtWnTQEWAtcAXQB/mpmK9z9QLBhBcPM\nWhLqcfhVLNog6EQUzTx2EmZmvYBngKvd/ZS5/CQiHcgMJ6EE4BozK3D3JcGGVefkAvnufhg4bGbv\nAymAEtGpJgKzwlOXZZvZdqAb8GGwYdU+M2tCKAm94O6vlFKk0tf1oLvmlgI3hUdZ/BjY7+67Ao6p\nTjKz84FXgBv1F2v53L2zu3dy906E1sf6hZJQqV4DLjGzxmbWHLiIUJ+/nOoLQneOmNk5hGaa3hZo\nRAEIPyP7M7DZ3R8po1ilr+s1ekdkZhmERpgkmFku8ADQBMDdnyI0bdA1QDZwhNBfHQ1SFG11P6E1\nnp4M/6Vf0FBnBI6irYSK28ndN5vZm8B64CTwjLs3yKVZovid+j0w38w2AEao67chLg3RH7gR2GBm\na8P77gHOh6pf1zXFj4iIBCrorjkREWnglIhERCRQSkQiIhIoJSIREQmUEpGIiARKiUhERAKlRCQi\nIoH6/1OJFuy/SC1PAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117ac50b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.222\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/workspace/CarND/CarND-Project2/CarND-Traffic-Sign-Classifier-Project/LeNet_eval.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mshow_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_data_dict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'my_test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspace/CarND/CarND-Project2/CarND-Traffic-Sign-Classifier-Project/LeNet_eval.ipynb\u001b[0m in \u001b[0;36mshow_data\u001b[0;34m(all_data, data_suffix_key)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_suffix_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Showing images from '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dataset for '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_suffix_key\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' data'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mfeature_label_original_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_feature_label_ranked_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_suffix_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_suffix_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mshow_sample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_label_original_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_label_original_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignnames_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_random\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_label_original_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_title\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCMAP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cmap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'name'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done for cipher10!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#%%capture output\\nfor index, data_dict in enumerate(all_data):\\n    current_data_dict = data_dict\\n    current_data_dict_name = data_dict[\\'name\\']\\n    print(\"Working on \" + current_data_dict_name + \"...\")\\n    %store current_data_dict\\n    %store current_data_dict_name\\n    %run LeNet_eval.ipynb \\'current_data_dict\\'\\n    print(\"All done for \" + current_data_dict_name + \"!\")\\n    print(\"###############################################################################\"+ \"\\n\\n\\n\\n\\n\\n\\n\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%capture output\n",
    "for index, data_dict in enumerate(all_data):\n",
    "    current_data_dict = data_dict\n",
    "    current_data_dict_name = data_dict['name']\n",
    "    print(\"Working on \" + current_data_dict_name + \"...\")\n",
    "    %store current_data_dict\n",
    "    %store current_data_dict_name\n",
    "    %run LeNet_eval.ipynb 'current_data_dict'\n",
    "    print(\"All done for \" + current_data_dict_name + \"!\")\n",
    "    print(\"###############################################################################\"+ \"\\n\\n\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0a4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
