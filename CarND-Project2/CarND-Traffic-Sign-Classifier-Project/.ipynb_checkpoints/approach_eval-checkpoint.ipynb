{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-e6004c38bcbe>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e6004c38bcbe>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    for test_image_filename in test_images[]:\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Test images analysis\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "import collections\n",
    "import math\n",
    "    \n",
    "def build_my_test():\n",
    "    test_images = listdir('./test_images/')\n",
    "    test_images_array = []\n",
    "    test_images_labels_array = []\n",
    "    for test_image_filename in test_images:\n",
    "        img = Image.open('./test_images/' + test_image_filename).convert('RGB')\n",
    "        img = img.resize((32,32), PIL.Image.ANTIALIAS)\n",
    "        test_images_array.append(np.asarray(img))\n",
    "        test_images_labels_array.append((test_image_filename.split('_')[1].split('.')[0]))\n",
    "        \n",
    "#     print(test_images_array)\n",
    "#     print(test_images_labels_array)\n",
    "    return (test_images_array, test_images_labels_array)\n",
    "\n",
    "import random\n",
    "\n",
    "def show_sample_images(Xs, count, signnames_dict, make_random=True, labels=None, fig_title=None, CMAP=None):\n",
    "    col_count = 5\n",
    "    rows = math.ceil(float(len(Xs)) / float(col_count))\n",
    "    fig = plt.figure(figsize=(4*int(col_count),2*rows))\n",
    "    if fig_title:\n",
    "        fig.suptitle(fig_title, fontsize=8)\n",
    "    for i in range(count):\n",
    "        if make_random:\n",
    "            index = random.randint(0, len(Xs)-1)\n",
    "            image = Xs[index].squeeze()\n",
    "        else:\n",
    "            image = Xs[i].squeeze()\n",
    "        ax1 = fig.add_subplot(rows,col_count,i+1)\n",
    "        ax1.set_title(labels[i] + ' occurences', fontsize=8)\n",
    "        ax1.set_xticks([]) \n",
    "        ax1.set_yticks([]) \n",
    "        ax1.imshow(image, cmap=CMAP)\n",
    "\n",
    "def build_feature_label_ranked_dict(X, Y):\n",
    "    assert(len(X) == len(Y))\n",
    "    label_feature_dict = {}\n",
    "    for x,y in zip(X, Y):\n",
    "        if y not in label_feature_dict:\n",
    "            label_feature_dict[y] = x\n",
    "    \n",
    "    label_count_dict = defaultdict(int)\n",
    "    for x,y in zip(X, Y):\n",
    "        label_count_dict[y] += 1\n",
    "    \n",
    "    ranked_count_to_label_feature_dict = collections.OrderedDict()\n",
    "    for label, count in sorted(label_count_dict.items(), key=lambda x:x[1]):\n",
    "        ranked_count_to_label_feature_dict[str(signnames_dict[str(label)]) + \": \" + str(count)] = label_feature_dict[label]\n",
    "    \n",
    "    return ranked_count_to_label_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import csv\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "training_file = DATA_DIR + \"traffic-signs-data/train.p\"\n",
    "validation_file = DATA_DIR + \"traffic-signs-data/valid.p\"\n",
    "testing_file = DATA_DIR + \"traffic-signs-data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "(X_my_test, y_my_test) = build_my_test()    \n",
    "    \n",
    "X_train, X_train_coords, X_train_sizes, y_train = np.asarray(train['features'], dtype=np.float32), train['coords'], train['sizes'], train['labels']\n",
    "X_valid, X_valid_coords,X_valid_sizes, y_valid = np.asarray(valid['features'], dtype=np.float32), valid['coords'], valid['sizes'], valid['labels']\n",
    "X_test, X_test_coords, X_test_sizes, y_test = np.asarray(test['features'], dtype=np.float32), test['coords'], test['sizes'], test['labels']\n",
    "\n",
    "# for k,v in build_feature_label_ranked_dict(X_train, y_train):\n",
    "#     print (k)\n",
    "#     print (v)\n",
    "#################\n",
    "\n",
    "### Replace each question mark with the appropriate value. \n",
    "### Use python, pandas or numpy methods rather than hard coding the results\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: Number of validation examples\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = X_train.shape[1:]\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformFeatures(transformFunc, XFeatures):\n",
    "    return np.array([transformFunc(X_image) for X_image in XFeatures])\n",
    "\n",
    "def saveAndRetrievePickle(transformFunc, featuresDict, mode, pickleFileName):\n",
    "    if mode == 'saveAndRetrieve':\n",
    "        X_train_features = np.array([transformFunc(X_train_image) for X_train_image in featuresDict['X_train']])\n",
    "        X_valid_features = np.array([transformFunc(X_valid_image) for X_valid_image in featuresDict['X_valid']])\n",
    "        X_test_features = np.array([transformFunc(X_test_image) for X_test_image in featuresDict['X_test']])\n",
    "             \n",
    "        new_features_dict = {'X_train': X_train_features, 'y_train': y_train, 'X_valid': X_valid_features,\\\n",
    "                            'y_valid': y_valid, 'X_test': X_test_features, 'y_test': y_test}\n",
    "    \n",
    "        with open(DATA_DIR + pickleFileName + '.pickle', 'wb') as handle:\n",
    "            pickle.dump(new_features_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      \n",
    "    if mode == 'saveAndRetrieve' or mode == 'retrieve':\n",
    "        with open(DATA_DIR + pickleFileName + '.pickle', 'rb') as handle:\n",
    "            new_features_dict = pickle.load(handle)\n",
    "    \n",
    "    return new_features_dict\n",
    "            \n",
    "   \n",
    "# Original_data\n",
    "def noop_image(image):\n",
    "    return np.asarray(image, dtype=np.float32) \n",
    "\n",
    "print(\"# Prepping Original_data\")\n",
    "original_data = {'X_train': X_train, 'y_train': y_train, 'X_valid': X_valid, 'y_valid': y_valid, 'X_test': X_test, 'y_test': y_test}\n",
    "\n",
    "# original_data = saveAndRetrievePickle(noop_image, original_data, 'saveAndRetrieve', 'original_data')\n",
    "original_data = saveAndRetrievePickle(noop_image, original_data, 'retrieve', 'original_data') \n",
    "\n",
    "## My data\n",
    "original_data['X_my_test'] = transformFeatures(noop_image, X_my_test)\n",
    "original_data['y_my_test'] = y_my_test\n",
    "##\n",
    "\n",
    "# Create cropped data\n",
    "#TODO(saajan): Reconsider zeroing out irrelevant area over resizing cropped image\n",
    "def extract_bounds_and_rescale(image, coord, size):\n",
    "    transformed_x = 32\n",
    "    transformed_y = 32\n",
    "    original_x = size[0]\n",
    "    original_y = size[1]\n",
    "      \n",
    "    x_multiplier = float(transformed_x)/float(original_x)\n",
    "    y_multiplier = float(transformed_y)/float(original_y)\n",
    "      \n",
    "    transformed_coord = (coord[0]* x_multiplier, coord[1] * y_multiplier, coord[2] * x_multiplier, coord[3] * y_multiplier)\n",
    "    transformed_coord = [int(np.rint(val)) for val in transformed_coord]\n",
    "      \n",
    "    ret_image = image.copy()\n",
    "    shape = image.shape\n",
    "     \n",
    "    ret_image[0:transformed_coord[0],:] = (0,0,0)\n",
    "    ret_image[:,0:transformed_coord[1]] = (0,0,0)\n",
    "    ret_image[transformed_coord[2]:shape[1],:] = (0,0,0)\n",
    "    ret_image[:,transformed_coord[3]:shape[0]] = (0,0,0)\n",
    "    #show_sample_images([ret_image], 1)\n",
    "    return np.asarray(ret_image, dtype=np.float32)\n",
    "\n",
    "\n",
    "# # extract_bounds_and_rescale Xs\n",
    "# X_train = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_train, X_train_coords, X_train_sizes)])\n",
    "# X_valid = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_valid, X_valid_coords,X_valid_sizes)])\n",
    "# X_test = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_test, X_test_coords, X_test_sizes)])\n",
    "  \n",
    "print(\"# Prepping Cropped_data\")\n",
    "# cropped_data = {'X_train': X_train, 'y_train': y_train, 'X_valid': X_valid, 'y_valid': y_valid, 'X_test': X_test, 'y_test': y_test}\n",
    "# with open(DATA_DIR + 'cropped_data.pickle', 'wb') as cropped_data_handle:\n",
    "#     pickle.dump(cropped_data, cropped_data_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "with open(DATA_DIR + 'cropped_data.pickle', 'rb') as cropped_data_handle:\n",
    "    cropped_data = pickle.load(cropped_data_handle)\n",
    "     \n",
    "## My data -- noop\n",
    "cropped_data['X_my_test'] = X_my_test\n",
    "cropped_data['y_my_test'] = y_my_test\n",
    "##\n",
    "# show_sample_images(cropped_data['X_train'], 10)\n",
    "\n",
    "\n",
    "# normalize Xs\n",
    "def normalize(image, mean_pixel):\n",
    "    result = (np.asarray(image, dtype=np.float32) - mean_pixel) / mean_pixel\n",
    "    #normalizer_func = np.vectorize(lambda val: (float(val)-float(mean_pixel))/float(mean_pixel))\n",
    "    return np.asarray(result, dtype=np.float32)\n",
    "\n",
    "print(\"# Prepping Original_normalized_data\")\n",
    "# original_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in original_data['X_train']])), original_data, 'saveAndRetrieve', 'original_normalized_data')\n",
    "original_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in original_data['X_train']])), original_data, 'retrieve', 'original_normalized_data')\n",
    "## My data\n",
    "original_normalized_data['X_my_test'] = transformFeatures(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in original_data['X_my_test']])), original_data['X_my_test'])\n",
    "original_normalized_data['y_my_test'] = y_my_test\n",
    "##\n",
    "\n",
    "print(\"# Prepping Cropped_normalized_data\")\n",
    "# cropped_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_data['X_train']])), cropped_data, 'saveAndRetrieve', 'cropped_normalized_data')\n",
    "cropped_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_data['X_train']])), cropped_data, 'retrieve', 'cropped_normalized_data')\n",
    "## My data\n",
    "cropped_normalized_data['X_my_test'] = transformFeatures(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_data['X_my_test']])), cropped_data['X_my_test'])\n",
    "cropped_normalized_data['y_my_test'] = y_my_test\n",
    "##  \n",
    "\n",
    "# # Experiments\n",
    "# vals = []\n",
    "# for image in cropped_normalized_data['X_train']:\n",
    "#     vals.append((np.mean(image)))\n",
    "# print(np.mean(vals))    \n",
    "\n",
    "\n",
    "import cv2\n",
    "# convert_to_grayscale Xs\n",
    "def convert_to_grayscale(image):\n",
    "    return np.asarray(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY), dtype=np.float32).reshape((32, 32, 1))\n",
    "    \n",
    "print(\"# Prepping Cropped_grayscale_data\")\n",
    "# cropped_grayscale_data = saveAndRetrievePickle(convert_to_grayscale, cropped_data, 'saveAndRetrieve', 'cropped_grayscale_data')\n",
    "cropped_grayscale_data = saveAndRetrievePickle(convert_to_grayscale, cropped_data, 'retrieve', 'cropped_grayscale_data') \n",
    "## My data\n",
    "cropped_grayscale_data['X_my_test'] = transformFeatures(convert_to_grayscale, cropped_data['X_my_test'])\n",
    "cropped_grayscale_data['y_my_test'] = y_my_test\n",
    "##\n",
    "\n",
    "print(\"# Prepping Cropped_grayscaled_normalized_data\")\n",
    "# cropped_grayscaled_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_grayscale_data['X_train']])), cropped_grayscale_data, 'saveAndRetrieve', 'cropped_grayscaled_normalized_data')\n",
    "cropped_grayscaled_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_grayscale_data['X_train']])), cropped_grayscale_data, 'retrieve', 'cropped_grayscaled_normalized_data') \n",
    "## My data\n",
    "cropped_grayscaled_normalized_data['X_my_test'] = transformFeatures(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_grayscale_data['X_my_test']])), cropped_grayscale_data['X_my_test'])\n",
    "cropped_grayscaled_normalized_data['y_my_test'] = y_my_test\n",
    "##\n",
    "\n",
    "#Naming\n",
    "original_data['name'] = 'original_data'\n",
    "original_normalized_data['name'] = 'original_normalized_data'\n",
    "cropped_data['name'] = 'cropped_data'\n",
    "cropped_grayscale_data['name'] = 'cropped_grayscale_data'\n",
    "cropped_normalized_data['name'] = 'cropped_normalized_data'\n",
    "cropped_grayscaled_normalized_data['name'] = 'cropped_grayscaled_normalized_data'\n",
    "\n",
    "#cmap\n",
    "original_data['cmap'] = None\n",
    "original_normalized_data['cmap'] = None\n",
    "cropped_data['cmap'] = None\n",
    "cropped_grayscale_data['cmap'] = 'gray'\n",
    "cropped_normalized_data['cmap'] = None\n",
    "cropped_grayscaled_normalized_data['cmap'] = 'gray'\n",
    "all_data = [original_data, original_normalized_data, cropped_data, cropped_normalized_data, cropped_grayscale_data, cropped_grayscaled_normalized_data] \n",
    "#all_data = [original_normalized_data, cropped_grayscale_data] \n",
    "#all_data = [original_normalized_data] \n",
    "\n",
    "print('Done with data prep!')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "\n",
    "##################\n",
    "\n",
    "\n",
    "### Data exploration visualization code goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "# Visualizations will be shown in the notebook.\n",
    "#TODO(saajan): uncomment line below and plot and do more visualization\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "#bins = np.linspace(-5, 5, 25, endpoint=True)\n",
    "\n",
    "signnames_dict = {}\n",
    "signnames = [None]\n",
    "with open('./signnames.csv') as signnamescsv:\n",
    "    signnamesreader = csv.reader(signnamescsv, delimiter=',')\n",
    "    for label, name in signnamesreader:\n",
    "        signnames.append(name)\n",
    "        signnames_dict[label] = name\n",
    "ax.hist(y_train, alpha=0.5, label='training_labels', bins=43)\n",
    "ax.hist(y_valid, alpha=0.5, label='validation_labels', bins=43)\n",
    "ax.hist(y_test, alpha=0.5, label='test_labels', bins=43)\n",
    "ind = np.arange(43)  # the x locations for the groups\n",
    "width = 0\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(tuple(signnames[1:]))\n",
    "#ax.tick_params(axis='x', which='major', labelsize=10, direction='')\n",
    "#ax.tick_params(axis='x', direction='out', colors='r')\n",
    "plt.xticks(rotation=90, linespacing=0, size=8)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "###################\n",
    "        #ax1.imshow(image, cmap=\"gray\")\n",
    "        \n",
    "        #plt.imshow(image, cmap=\"gray\")\n",
    "        #plt.imshow(image)\n",
    "           \n",
    "# show_sample_images(X_train, 10)\n",
    "\n",
    "### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n",
    "### converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample images for each class and each transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_data(all_data, data_suffix_key):\n",
    "    for data_dict in all_data:\n",
    "        title = 'Showing images from ' + data_dict['name'] + ' dataset for ' + data_suffix_key + ' data' + \":\"\n",
    "        feature_label_original_data = build_feature_label_ranked_dict(data_dict['X_' + data_suffix_key], data_dict['y_' + data_suffix_key])\n",
    "        show_sample_images(list(feature_label_original_data.values()), len(list(feature_label_original_data.values())), signnames_dict, make_random=False, labels=list(feature_label_original_data.keys()), fig_title=title, CMAP=data_dict['cmap'])\n",
    "    \n",
    "\n",
    "show_data(all_data, 'train')\n",
    "show_data(all_data, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this as a refernce for recognizing keys to datasets:\n",
    "#### original_data => Original images\n",
    "#### original_normalized_data => Normalized images\n",
    "#### cropped_data => Cropped images\n",
    "#### cropped_grayscale_data => Cropped grayscaled images\n",
    "#### cropped_normalized_data => Cropped normalized images\n",
    "#### cropped_grayscaled_normalized_data => Cropped grayscaled normalized images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model generation and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%capture output\n",
    "for index, data_dict in enumerate(all_data):\n",
    "    current_data_dict = data_dict\n",
    "    current_data_dict_name = data_dict['name']\n",
    "    print(\"Working on \" + current_data_dict_name + \"...\")\n",
    "    %store current_data_dict\n",
    "    %store current_data_dict_name\n",
    "    %run LeNet_eval.ipynb 'current_data_dict'\n",
    "    print(\"All done for \" + current_data_dict_name + \"!\")\n",
    "    print(\"###############################################################################\"+ \"\\n\\n\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
