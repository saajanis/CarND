{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 34799\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n",
      "# Original_data\n",
      "# Original_normalized_data\n",
      "82.6776\n",
      "# Cropped_normalized_data\n",
      "48.38\n",
      "1.14006e-08\n",
      "# Cropped_grayscale_data\n",
      "# Cropped_grayscaled_normalized_data\n",
      "47.6308\n",
      "Done with data prep!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "training_file = DATA_DIR + \"traffic-signs-data/train.p\"\n",
    "validation_file = DATA_DIR + \"traffic-signs-data/valid.p\"\n",
    "testing_file = DATA_DIR + \"traffic-signs-data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, X_train_coords, X_train_sizes, y_train = np.asarray(train['features'], dtype=np.float32), train['coords'], train['sizes'], train['labels']\n",
    "X_valid, X_valid_coords,X_valid_sizes, y_valid = np.asarray(valid['features'], dtype=np.float32), valid['coords'], valid['sizes'], valid['labels']\n",
    "X_test, X_test_coords, X_test_sizes, y_test = np.asarray(test['features'], dtype=np.float32), test['coords'], test['sizes'], test['labels']\n",
    "\n",
    "#################\n",
    "\n",
    "### Replace each question mark with the appropriate value. \n",
    "### Use python, pandas or numpy methods rather than hard coding the results\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: Number of validation examples\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = X_train.shape[1:]\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "\n",
    "##################\n",
    "\n",
    "\n",
    "### Data exploration visualization code goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "# Visualizations will be shown in the notebook.\n",
    "#TODO(saajan): uncomment line below and plot and do more visualization\n",
    "# %matplotlib inline\n",
    " \n",
    "# plt.hist(y_train, alpha=0.5, label='training_labels', bins=43)\n",
    "# plt.hist(y_valid, alpha=0.5, label='validation_labels', bins=43)\n",
    "# plt.hist(y_test, alpha=0.5, label='test_labels', bins=43)\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()\n",
    "\n",
    "###################\n",
    "import random\n",
    "\n",
    "def show_sample_images(Xs, count):\n",
    "    fig = plt.figure()\n",
    "    for i in range(count):\n",
    "        index = random.randint(0, len(Xs)-1)\n",
    "        image = Xs[index].squeeze()\n",
    "        ax1 = fig.add_subplot(1,count,i+1)\n",
    "        ax1.imshow(image)\n",
    "        #ax1.imshow(image, cmap=\"gray\")\n",
    "        \n",
    "        #plt.imshow(image, cmap=\"gray\")\n",
    "        #plt.imshow(image)\n",
    "           \n",
    "# show_sample_images(X_train, 10)\n",
    "\n",
    "### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n",
    "### converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed.    \n",
    "\n",
    "def saveAndRetrievePickle(transformFunc, featuresDict, mode, pickleFileName):\n",
    "    if mode == 'saveAndRetrieve':\n",
    "        X_train_features = np.array([transformFunc(X_train_image) for X_train_image in featuresDict['X_train']])\n",
    "        X_valid_features = np.array([transformFunc(X_valid_image) for X_valid_image in featuresDict['X_valid']])\n",
    "        X_test_features = np.array([transformFunc(X_test_image) for X_test_image in featuresDict['X_test']])\n",
    "             \n",
    "        new_features_dict = {'X_train': X_train_features, 'y_train': y_train, 'X_valid': X_valid_features,\\\n",
    "                            'y_valid': y_valid, 'X_test': X_test_features, 'y_test': y_test}\n",
    "    \n",
    "        with open(DATA_DIR + pickleFileName + '.pickle', 'wb') as handle:\n",
    "            pickle.dump(new_features_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      \n",
    "    if mode == 'saveAndRetrieve' or mode == 'retrieve':\n",
    "        with open(DATA_DIR + pickleFileName + '.pickle', 'rb') as handle:\n",
    "            new_features_dict = pickle.load(handle)\n",
    "    \n",
    "    return new_features_dict\n",
    "            \n",
    "   \n",
    "# Original_data\n",
    "def noop_image(image):\n",
    "    return np.asarray(image, dtype=np.float32) \n",
    "\n",
    "print(\"# Original_data\")\n",
    "original_data = {'X_train': X_train, 'y_train': y_train, 'X_valid': X_valid, 'y_valid': y_valid, 'X_test': X_test, 'y_test': y_test}\n",
    "# original_data = saveAndRetrievePickle(noop_image, original_data, 'saveAndRetrieve', 'original_data')\n",
    "original_data = saveAndRetrievePickle(noop_image, original_data, 'retrieve', 'original_data') \n",
    "\n",
    "\n",
    "# Create cropped data\n",
    "#TODO(saajan): Reconsider zeroing out irrelevant area over resizing cropped image\n",
    "def extract_bounds_and_rescale(image, coord, size):\n",
    "    transformed_x = 32\n",
    "    transformed_y = 32\n",
    "    original_x = size[0]\n",
    "    original_y = size[1]\n",
    "      \n",
    "    x_multiplier = float(transformed_x)/float(original_x)\n",
    "    y_multiplier = float(transformed_y)/float(original_y)\n",
    "      \n",
    "    transformed_coord = (coord[0]* x_multiplier, coord[1] * y_multiplier, coord[2] * x_multiplier, coord[3] * y_multiplier)\n",
    "    transformed_coord = [int(np.rint(val)) for val in transformed_coord]\n",
    "      \n",
    "    ret_image = image.copy()\n",
    "    shape = image.shape\n",
    "     \n",
    "    ret_image[0:transformed_coord[0],:] = (0,0,0)\n",
    "    ret_image[:,0:transformed_coord[1]] = (0,0,0)\n",
    "    ret_image[transformed_coord[2]:shape[1],:] = (0,0,0)\n",
    "    ret_image[:,transformed_coord[3]:shape[0]] = (0,0,0)\n",
    "    #show_sample_images([ret_image], 1)\n",
    "    return np.asarray(ret_image, dtype=np.float32)\n",
    "\n",
    "\n",
    "# # extract_bounds_and_rescale Xs\n",
    "# X_train = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_train, X_train_coords, X_train_sizes)])\n",
    "# X_valid = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_valid, X_valid_coords,X_valid_sizes)])\n",
    "# X_test = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_test, X_test_coords, X_test_sizes)])\n",
    "  \n",
    "print(\"# Cropped_data\")\n",
    "# cropped_data = {'X_train': X_train, 'y_train': y_train, 'X_valid': X_valid, 'y_valid': y_valid, 'X_test': X_test, 'y_test': y_test}\n",
    "# with open(DATA_DIR + 'cropped_data.pickle', 'wb') as cropped_data_handle:\n",
    "#     pickle.dump(cropped_data, cropped_data_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "with open(DATA_DIR + 'cropped_data.pickle', 'rb') as cropped_data_handle:\n",
    "    cropped_data = pickle.load(cropped_data_handle)\n",
    "     \n",
    "# show_sample_images(cropped_data['X_train'], 10)\n",
    "\n",
    "\n",
    "# normalize Xs\n",
    "def normalize(image, mean_pixel):\n",
    "    result = (np.asarray(image, dtype=np.float32) - mean_pixel) / mean_pixel\n",
    "    #normalizer_func = np.vectorize(lambda val: (float(val)-float(mean_pixel))/float(mean_pixel))\n",
    "    return np.asarray(result, dtype=np.float32)\n",
    "\n",
    "print(\"# Original_normalized_data\")\n",
    "# original_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in original_data['X_train']])), original_data, 'saveAndRetrieve', 'original_normalized_data')\n",
    "original_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in original_data['X_train']])), original_data, 'retrieve', 'original_normalized_data')\n",
    "\n",
    "print(\"# Cropped_normalized_data\")\n",
    "# cropped_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_data['X_train']])), cropped_data, 'saveAndRetrieve', 'cropped_normalized_data')\n",
    "cropped_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_data['X_train']])), cropped_data, 'retrieve', 'cropped_normalized_data')\n",
    "   \n",
    "# # Experiments\n",
    "# vals = []\n",
    "# for image in cropped_normalized_data['X_train']:\n",
    "#     vals.append((np.mean(image)))\n",
    "# print(np.mean(vals))    \n",
    "\n",
    "\n",
    "import cv2\n",
    "# convert_to_grayscale Xs\n",
    "def convert_to_grayscale(image):\n",
    "    return np.asarray(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY), dtype=np.float32).reshape((32, 32, 1))\n",
    "    \n",
    "print(\"# Cropped_grayscale_data\")\n",
    "# cropped_grayscale_data = saveAndRetrievePickle(convert_to_grayscale, cropped_data, 'saveAndRetrieve', 'cropped_grayscale_data')\n",
    "cropped_grayscale_data = saveAndRetrievePickle(convert_to_grayscale, cropped_data, 'retrieve', 'cropped_grayscale_data') \n",
    "\n",
    "print(\"# Cropped_grayscaled_normalized_data\")\n",
    "# cropped_grayscaled_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_grayscale_data['X_train']])), cropped_grayscale_data, 'saveAndRetrieve', 'cropped_grayscaled_normalized_data')\n",
    "cropped_grayscaled_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_grayscale_data['X_train']])), cropped_grayscale_data, 'retrieve', 'cropped_grayscaled_normalized_data') \n",
    "\n",
    "#Naming\n",
    "original_data['name'] = 'original_data'\n",
    "original_normalized_data['name'] = 'original_normalized_data'\n",
    "cropped_data['name'] = 'cropped_data'\n",
    "cropped_grayscale_data['name'] = 'cropped_grayscale_data'\n",
    "cropped_normalized_data['name'] = 'cropped_normalized_data'\n",
    "cropped_grayscaled_normalized_data['name'] = 'cropped_grayscaled_normalized_data'\n",
    "#all_data = [original_data, original_normalized_data, cropped_data, cropped_normalized_data, cropped_grayscale_data, cropped_grayscaled_normalized_data] \n",
    "#all_data = [original_normalized_data, cropped_grayscale_data] \n",
    "all_data = [original_normalized_data] \n",
    "\n",
    "print('Done with data prep!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'current_data_dict' (dict)\n",
      "Stored 'current_data_dict_name' (str)\n",
      "\n",
      "Image Shape: (32, 32, 3)\n",
      "\n",
      "Training Set:   34799 samples\n",
      "Validation Set: 4410 samples\n",
      "Test Set:       12630 samples\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/1 [00:00<?, ?epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "Model saved at: ./data/original_normalized_data_0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:38<00:00, 38.33s/epochs]\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py:2917: UserWarning: Attempting to set identical left==right results\n",
      "in singular transformations; automatically expanding.\n",
      "left=1, right=1\n",
      "  'left=%s, right=%s') % (left, right))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAACeCAYAAABwxA4NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCQiIpH6/+PdNJ0XwuoAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15746f630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracies: \n",
      "[0.83669070958707514]\n",
      "Validation accuracies: \n",
      "[0.7485260774489163]\n",
      "INFO:tensorflow:Restoring parameters from ./data/original_normalized_data_0\n",
      "Test Accuracy = 0.730\n",
      "All done...!\n"
     ]
    }
   ],
   "source": [
    "#%%capture output\n",
    "for index, data_dict in enumerate(all_data):\n",
    "    current_data_dict = data_dict\n",
    "    current_data_dict_name = data_dict['name']\n",
    "    %store current_data_dict\n",
    "    %store current_data_dict_name\n",
    "    %run LeNet_eval.ipynb 'current_data_dict'\n",
    "print(\"All done...!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0a4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
