{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 34799\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n",
      "# Original_data\n",
      "# Original_normalized_data\n",
      "82.6776\n",
      "# Cropped_normalized_data\n",
      "48.38\n",
      "1.14006e-08\n",
      "# Cropped_grayscale_data\n",
      "# Cropped_grayscaled_normalized_data\n",
      "47.6308\n",
      "Done with data prep!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "training_file = DATA_DIR + \"traffic-signs-data/train.p\"\n",
    "validation_file = DATA_DIR + \"traffic-signs-data/valid.p\"\n",
    "testing_file = DATA_DIR + \"traffic-signs-data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, X_train_coords, X_train_sizes, y_train = np.asarray(train['features'], dtype=np.float32), train['coords'], train['sizes'], train['labels']\n",
    "X_valid, X_valid_coords,X_valid_sizes, y_valid = np.asarray(valid['features'], dtype=np.float32), valid['coords'], valid['sizes'], valid['labels']\n",
    "X_test, X_test_coords, X_test_sizes, y_test = np.asarray(test['features'], dtype=np.float32), test['coords'], test['sizes'], test['labels']\n",
    "\n",
    "#################\n",
    "\n",
    "### Replace each question mark with the appropriate value. \n",
    "### Use python, pandas or numpy methods rather than hard coding the results\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: Number of validation examples\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = X_train.shape[1:]\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "\n",
    "##################\n",
    "\n",
    "\n",
    "### Data exploration visualization code goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "# Visualizations will be shown in the notebook.\n",
    "#TODO(saajan): uncomment line below and plot and do more visualization\n",
    "# %matplotlib inline\n",
    " \n",
    "# plt.hist(y_train, alpha=0.5, label='training_labels', bins=43)\n",
    "# plt.hist(y_valid, alpha=0.5, label='validation_labels', bins=43)\n",
    "# plt.hist(y_test, alpha=0.5, label='test_labels', bins=43)\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()\n",
    "\n",
    "###################\n",
    "import random\n",
    "\n",
    "def show_sample_images(Xs, count):\n",
    "    fig = plt.figure()\n",
    "    for i in range(count):\n",
    "        index = random.randint(0, len(Xs)-1)\n",
    "        image = Xs[index].squeeze()\n",
    "        ax1 = fig.add_subplot(1,count,i+1)\n",
    "        ax1.imshow(image)\n",
    "        #ax1.imshow(image, cmap=\"gray\")\n",
    "        \n",
    "        #plt.imshow(image, cmap=\"gray\")\n",
    "        #plt.imshow(image)\n",
    "           \n",
    "# show_sample_images(X_train, 10)\n",
    "\n",
    "### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n",
    "### converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed.    \n",
    "\n",
    "def saveAndRetrievePickle(transformFunc, featuresDict, mode, pickleFileName):\n",
    "    if mode == 'saveAndRetrieve':\n",
    "        X_train_features = np.array([transformFunc(X_train_image) for X_train_image in featuresDict['X_train']])\n",
    "        X_valid_features = np.array([transformFunc(X_valid_image) for X_valid_image in featuresDict['X_valid']])\n",
    "        X_test_features = np.array([transformFunc(X_test_image) for X_test_image in featuresDict['X_test']])\n",
    "             \n",
    "        new_features_dict = {'X_train': X_train_features, 'y_train': y_train, 'X_valid': X_valid_features,\\\n",
    "                            'y_valid': y_valid, 'X_test': X_test_features, 'y_test': y_test}\n",
    "    \n",
    "        with open(DATA_DIR + pickleFileName + '.pickle', 'wb') as handle:\n",
    "            pickle.dump(new_features_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      \n",
    "    if mode == 'saveAndRetrieve' or mode == 'retrieve':\n",
    "        with open(DATA_DIR + pickleFileName + '.pickle', 'rb') as handle:\n",
    "            new_features_dict = pickle.load(handle)\n",
    "    \n",
    "    return new_features_dict\n",
    "            \n",
    "   \n",
    "# Original_data\n",
    "def noop_image(image):\n",
    "    return np.asarray(image, dtype=np.float32) \n",
    "\n",
    "print(\"# Original_data\")\n",
    "original_data = {'X_train': X_train, 'y_train': y_train, 'X_valid': X_valid, 'y_valid': y_valid, 'X_test': X_test, 'y_test': y_test}\n",
    "# original_data = saveAndRetrievePickle(noop_image, original_data, 'saveAndRetrieve', 'original_data')\n",
    "original_data = saveAndRetrievePickle(noop_image, original_data, 'retrieve', 'original_data') \n",
    "\n",
    "\n",
    "# Create cropped data\n",
    "#TODO(saajan): Reconsider zeroing out irrelevant area over resizing cropped image\n",
    "def extract_bounds_and_rescale(image, coord, size):\n",
    "    transformed_x = 32\n",
    "    transformed_y = 32\n",
    "    original_x = size[0]\n",
    "    original_y = size[1]\n",
    "      \n",
    "    x_multiplier = float(transformed_x)/float(original_x)\n",
    "    y_multiplier = float(transformed_y)/float(original_y)\n",
    "      \n",
    "    transformed_coord = (coord[0]* x_multiplier, coord[1] * y_multiplier, coord[2] * x_multiplier, coord[3] * y_multiplier)\n",
    "    transformed_coord = [int(np.rint(val)) for val in transformed_coord]\n",
    "      \n",
    "    ret_image = image.copy()\n",
    "    shape = image.shape\n",
    "     \n",
    "    ret_image[0:transformed_coord[0],:] = (0,0,0)\n",
    "    ret_image[:,0:transformed_coord[1]] = (0,0,0)\n",
    "    ret_image[transformed_coord[2]:shape[1],:] = (0,0,0)\n",
    "    ret_image[:,transformed_coord[3]:shape[0]] = (0,0,0)\n",
    "    #show_sample_images([ret_image], 1)\n",
    "    return np.asarray(ret_image, dtype=np.float32)\n",
    "\n",
    "\n",
    "# # extract_bounds_and_rescale Xs\n",
    "# X_train = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_train, X_train_coords, X_train_sizes)])\n",
    "# X_valid = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_valid, X_valid_coords,X_valid_sizes)])\n",
    "# X_test = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_test, X_test_coords, X_test_sizes)])\n",
    "  \n",
    "print(\"# Cropped_data\")\n",
    "# cropped_data = {'X_train': X_train, 'y_train': y_train, 'X_valid': X_valid, 'y_valid': y_valid, 'X_test': X_test, 'y_test': y_test}\n",
    "# with open(DATA_DIR + 'cropped_data.pickle', 'wb') as cropped_data_handle:\n",
    "#     pickle.dump(cropped_data, cropped_data_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "with open(DATA_DIR + 'cropped_data.pickle', 'rb') as cropped_data_handle:\n",
    "    cropped_data = pickle.load(cropped_data_handle)\n",
    "     \n",
    "# show_sample_images(cropped_data['X_train'], 10)\n",
    "\n",
    "\n",
    "# normalize Xs\n",
    "def normalize(image, mean_pixel):\n",
    "    result = (np.asarray(image, dtype=np.float32) - mean_pixel) / mean_pixel\n",
    "    #normalizer_func = np.vectorize(lambda val: (float(val)-float(mean_pixel))/float(mean_pixel))\n",
    "    return np.asarray(result, dtype=np.float32)\n",
    "\n",
    "print(\"# Original_normalized_data\")\n",
    "# original_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in original_data['X_train']])), original_data, 'saveAndRetrieve', 'original_normalized_data')\n",
    "original_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in original_data['X_train']])), original_data, 'retrieve', 'original_normalized_data')\n",
    "\n",
    "print(\"# Cropped_normalized_data\")\n",
    "# cropped_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_data['X_train']])), cropped_data, 'saveAndRetrieve', 'cropped_normalized_data')\n",
    "cropped_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_data['X_train']])), cropped_data, 'retrieve', 'cropped_normalized_data')\n",
    "   \n",
    "# # Experiments\n",
    "# vals = []\n",
    "# for image in cropped_normalized_data['X_train']:\n",
    "#     vals.append((np.mean(image)))\n",
    "# print(np.mean(vals))    \n",
    "\n",
    "\n",
    "import cv2\n",
    "# convert_to_grayscale Xs\n",
    "def convert_to_grayscale(image):\n",
    "    return np.asarray(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY), dtype=np.float32).reshape((32, 32, 1))\n",
    "    \n",
    "print(\"# Cropped_grayscale_data\")\n",
    "# cropped_grayscale_data = saveAndRetrievePickle(convert_to_grayscale, cropped_data, 'saveAndRetrieve', 'cropped_grayscale_data')\n",
    "cropped_grayscale_data = saveAndRetrievePickle(convert_to_grayscale, cropped_data, 'retrieve', 'cropped_grayscale_data') \n",
    "\n",
    "print(\"# Cropped_grayscaled_normalized_data\")\n",
    "# cropped_grayscaled_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_grayscale_data['X_train']])), cropped_grayscale_data, 'saveAndRetrieve', 'cropped_grayscaled_normalized_data')\n",
    "cropped_grayscaled_normalized_data = saveAndRetrievePickle(partial(normalize, mean_pixel=np.mean([np.mean(image) for image in cropped_grayscale_data['X_train']])), cropped_grayscale_data, 'retrieve', 'cropped_grayscaled_normalized_data') \n",
    "\n",
    "#Naming\n",
    "original_data['name'] = 'original_data'\n",
    "original_normalized_data['name'] = 'original_normalized_data'\n",
    "cropped_data['name'] = 'cropped_data'\n",
    "cropped_grayscale_data['name'] = 'cropped_grayscale_data'\n",
    "cropped_normalized_data['name'] = 'cropped_normalized_data'\n",
    "cropped_grayscaled_normalized_data['name'] = 'cropped_grayscaled_normalized_data'\n",
    "#all_data = [original_data, original_normalized_data, cropped_data, cropped_normalized_data, cropped_grayscale_data, cropped_grayscaled_normalized_data] \n",
    "#all_data = [original_normalized_data, cropped_grayscale_data] \n",
    "all_data = [original_normalized_data] \n",
    "\n",
    "print('Done with data prep!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'current_data_dict' (dict)\n",
      "Stored 'current_data_dict_name' (str)\n",
      "\n",
      "Image Shape: (32, 32, 3)\n",
      "\n",
      "Training Set:   34799 samples\n",
      "Validation Set: 4410 samples\n",
      "Test Set:       12630 samples\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/1 [00:00<?, ?epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exists. Restoring from: ./data/original_normalized_data_0...\n",
      "INFO:tensorflow:Restoring parameters from ./data/original_normalized_data_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:11<00:00, 11.17s/epochs]\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py:2917: UserWarning: Attempting to set identical left==right results\n",
      "in singular transformations; automatically expanding.\n",
      "left=1, right=1\n",
      "  'left=%s, right=%s') % (left, right))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAACeCAYAAABwxA4NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niUhERCKlRCQiIpFSIhIRkUj9f1ZJM8KXrEESAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x146e15208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracies: \n",
      "[0.73137159109828132]\n",
      "Validation accuracies: \n",
      "[0.62743764153413495]\n",
      "INFO:tensorflow:Restoring parameters from ./data/original_normalized_data_0\n",
      "Test Accuracy = 0.619\n",
      "All done...!\n"
     ]
    }
   ],
   "source": [
    "#%%capture output\n",
    "for index, data_dict in enumerate(all_data):\n",
    "    current_data_dict = data_dict\n",
    "    current_data_dict_name = data_dict['name']\n",
    "    %store current_data_dict\n",
    "    %store current_data_dict_name\n",
    "    %run LeNet_eval.ipynb 'current_data_dict'\n",
    "print(\"All done...!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0a4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
