{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 34799\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n",
      "Done with data prep!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load pickled data\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "training_file = DATA_DIR + \"traffic-signs-data/train.p\"\n",
    "validation_file = DATA_DIR + \"traffic-signs-data/valid.p\"\n",
    "testing_file = DATA_DIR + \"traffic-signs-data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, X_train_coords, X_train_sizes, y_train = np.asarray(train['features'], dtype=np.float32), train['coords'], train['sizes'], train['labels']\n",
    "X_valid, X_valid_coords,X_valid_sizes, y_valid = np.asarray(valid['features'], dtype=np.float32), valid['coords'], valid['sizes'], valid['labels']\n",
    "X_test, X_test_coords, X_test_sizes, y_test = np.asarray(test['features'], dtype=np.float32), test['coords'], test['sizes'], test['labels']\n",
    "\n",
    "#################\n",
    "\n",
    "### Replace each question mark with the appropriate value. \n",
    "### Use python, pandas or numpy methods rather than hard coding the results\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: Number of validation examples\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = X_train.shape[1:]\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "\n",
    "##################\n",
    "\n",
    "\n",
    "### Data exploration visualization code goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "# Visualizations will be shown in the notebook.\n",
    "#TODO(saajan): uncomment line below and plot and do more visualization\n",
    "# %matplotlib inline\n",
    " \n",
    "# plt.hist(y_train, alpha=0.5, label='training_labels', bins=43)\n",
    "# plt.hist(y_valid, alpha=0.5, label='validation_labels', bins=43)\n",
    "# plt.hist(y_test, alpha=0.5, label='test_labels', bins=43)\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()\n",
    "\n",
    "###################\n",
    "import random\n",
    "\n",
    "def show_sample_images(Xs, count):\n",
    "    fig = plt.figure()\n",
    "    for i in range(count):\n",
    "        index = random.randint(0, len(Xs)-1)\n",
    "        image = Xs[index].squeeze()\n",
    "        ax1 = fig.add_subplot(1,count,i+1)\n",
    "        ax1.imshow(image)\n",
    "        #ax1.imshow(image, cmap=\"gray\")\n",
    "        \n",
    "        #plt.imshow(image, cmap=\"gray\")\n",
    "        #plt.imshow(image)\n",
    "           \n",
    "# show_sample_images(X_train, 10)\n",
    "\n",
    "### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n",
    "### converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed.    \n",
    "\n",
    "def saveAndRetrievePickle(transformFunc, featuresDict, mode, pickleFileName):\n",
    "    if mode == 'saveAndRetrieve':\n",
    "        X_train_features = np.array([transformFunc(X_train_image) for X_train_image in featuresDict['X_train']])\n",
    "        X_valid_features = np.array([transformFunc(X_valid_image) for X_valid_image in featuresDict['X_valid']])\n",
    "        X_test_features = np.array([transformFunc(X_test_image) for X_test_image in featuresDict['X_test']])\n",
    "             \n",
    "        new_features_dict = {'X_train': X_train_features, 'y_train': y_train, 'X_valid': X_valid_features,\\\n",
    "                            'y_valid': y_valid, 'X_test': X_test_features, 'y_test': y_test}\n",
    "    \n",
    "        with open(DATA_DIR + pickleFileName + '.pickle', 'wb') as handle:\n",
    "            pickle.dump(new_features_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      \n",
    "    if mode == 'saveAndRetrieve' or mode == 'retrieve':\n",
    "        with open(DATA_DIR + pickleFileName + '.pickle', 'rb') as handle:\n",
    "            new_features_dict = pickle.load(handle)\n",
    "    \n",
    "    return new_features_dict\n",
    "            \n",
    "   \n",
    "# Original_data\n",
    "def noop_image(image):\n",
    "    return np.asarray(image, dtype=np.float32) \n",
    "\n",
    "original_data = {'X_train': X_train, 'y_train': y_train, 'X_valid': X_valid, 'y_valid': y_valid, 'X_test': X_test, 'y_test': y_test}\n",
    "# original_data = saveAndRetrievePickle(noop_image, original_data, 'saveAndRetrieve', 'original_data')\n",
    "original_data = saveAndRetrievePickle(noop_image, original_data, 'retrieve', 'original_data') \n",
    "\n",
    "\n",
    "# Create cropped data\n",
    "#TODO(saajan): Reconsider zeroing out irrelevant area over resizing cropped image\n",
    "def extract_bounds_and_rescale(image, coord, size):\n",
    "    transformed_x = 32\n",
    "    transformed_y = 32\n",
    "    original_x = size[0]\n",
    "    original_y = size[1]\n",
    "      \n",
    "    x_multiplier = float(transformed_x)/float(original_x)\n",
    "    y_multiplier = float(transformed_y)/float(original_y)\n",
    "      \n",
    "    transformed_coord = (coord[0]* x_multiplier, coord[1] * y_multiplier, coord[2] * x_multiplier, coord[3] * y_multiplier)\n",
    "    transformed_coord = [int(np.rint(val)) for val in transformed_coord]\n",
    "      \n",
    "    ret_image = image.copy()\n",
    "    shape = image.shape\n",
    "     \n",
    "    ret_image[0:transformed_coord[0],:] = (0,0,0)\n",
    "    ret_image[:,0:transformed_coord[1]] = (0,0,0)\n",
    "    ret_image[transformed_coord[2]:shape[1],:] = (0,0,0)\n",
    "    ret_image[:,transformed_coord[3]:shape[0]] = (0,0,0)\n",
    "    #show_sample_images([ret_image], 1)\n",
    "    return np.asarray(ret_image, dtype=np.float32)\n",
    "\n",
    "\n",
    "# # extract_bounds_and_rescale Xs\n",
    "# X_train = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_train, X_train_coords, X_train_sizes)])\n",
    "# X_valid = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_valid, X_valid_coords,X_valid_sizes)])\n",
    "# X_test = np.array([extract_bounds_and_rescale(image, coord, size) for (image, coord, size) in zip(X_test, X_test_coords, X_test_sizes)])\n",
    "#      \n",
    "# cropped_data = {'X_train': X_train, 'y_train': y_train, 'X_valid': X_valid, 'y_valid': y_valid, 'X_test': X_test, 'y_test': y_test}\n",
    "# with open(DATA_DIR + 'cropped_data.pickle', 'wb') as cropped_data_handle:\n",
    "#     pickle.dump(cropped_data, cropped_data_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "with open(DATA_DIR + 'cropped_data.pickle', 'rb') as cropped_data_handle:\n",
    "    cropped_data = pickle.load(cropped_data_handle)\n",
    "     \n",
    "# show_sample_images(cropped_data['X_train'], 10)\n",
    "\n",
    "\n",
    "# normalize Xs\n",
    "def normalize(image):\n",
    "    normalizer_func = np.vectorize(lambda val: np.float32((float(val)-128.0)/128.0))\n",
    "    return np.asarray(normalizer_func(image), dtype=np.float32)\n",
    "\n",
    "original_normalized_data = saveAndRetrievePickle(normalize, original_data, 'saveAndRetrieve', 'original_normalized_data')\n",
    "original_normalized_data = saveAndRetrievePickle(normalize, original_data, 'retrieve', 'original_normalized_data')\n",
    "\n",
    "# cropped_normalized_data = saveAndRetrievePickle(normalize, cropped_data, 'saveAndRetrieve', 'cropped_normalized_data')\n",
    "cropped_normalized_data = saveAndRetrievePickle(normalize, cropped_data, 'retrieve', 'cropped_normalized_data')\n",
    "   \n",
    "# Experiments\n",
    "# vals = []\n",
    "# for image in cropped_normalized_data['X_train']:\n",
    "#     index = random.randint(0, len(cropped_normalized_data['X_train'])-1)\n",
    "#     vals.append((np.mean(cropped_normalized_data['X_train'][index])))\n",
    "# print(np.mean(vals))    \n",
    "\n",
    "\n",
    "import cv2\n",
    "# convert_to_grayscale Xs\n",
    "def convert_to_grayscale(image):\n",
    "    return np.asarray(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY), dtype=np.float32).reshape((32, 32, 1))\n",
    "    \n",
    "# cropped_grayscale_data = saveAndRetrievePickle(convert_to_grayscale, cropped_data, 'saveAndRetrieve', 'cropped_grayscale_data')\n",
    "cropped_grayscale_data = saveAndRetrievePickle(convert_to_grayscale, cropped_data, 'retrieve', 'cropped_grayscale_data') \n",
    "\n",
    "# Cropped grayscale normalized\n",
    "# cropped_grayscaled_normalized_data = saveAndRetrievePickle(normalize, cropped_grayscale_data, 'saveAndRetrieve', 'cropped_grayscaled_normalized_data')\n",
    "cropped_grayscaled_normalized_data = saveAndRetrievePickle(normalize, cropped_grayscale_data, 'retrieve', 'cropped_grayscaled_normalized_data') \n",
    "\n",
    "\n",
    "all_data = [original_data, original_normalized_data, cropped_data, cropped_normalized_data, cropped_grayscale_data, cropped_grayscaled_normalized_data] \n",
    "\n",
    "print('Done with data prep!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'current_data_dict' (dict)\n",
      "\n",
      "Image Shape: (32, 32, 3)\n",
      "\n",
      "Training Set:   34799 samples\n",
      "Validation Set: 4410 samples\n",
      "Test Set:       12630 samples\n",
      "(32, 32, 3)\n",
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.649\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.816\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.865\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.868\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.881\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.871\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.898\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.879\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.883\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.893\n",
      "\n",
      "Model saved\n",
      "INFO:tensorflow:Restoring parameters from ./lenet\n",
      "Test Accuracy = 0.889\n"
     ]
    }
   ],
   "source": [
    "for index, data_dict in enumerate(all_data):\n",
    "    current_data_dict = data_dict\n",
    "    %store current_data_dict\n",
    "    %run LeNet_eval.ipynb 'current_data_dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0a4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
